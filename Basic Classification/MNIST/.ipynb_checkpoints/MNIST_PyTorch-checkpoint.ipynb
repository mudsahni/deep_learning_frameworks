{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "Classify MNIST dataset using a deep neural network (on Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training images:  50000\n",
      "Num validation images:  10000\n",
      "Num test images:  10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "# get the training datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "train_len = 50000 #int(len(train_data)*0.85)\n",
    "valid_len = 10000 #int(len(train_data)*0.15)\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [train_len,valid_len])\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                download=True, transform = transform)\n",
    "\n",
    "# prepare data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "\n",
    "loaders = {'train':train_loader, 'valid':valid_loader, 'test':test_loader}\n",
    "# print out some data stats\n",
    "print('Num training images: ', len(train_data))\n",
    "print('Num validation images: ', len(valid_data))\n",
    "print('Num test images: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# display 20 images\n",
    "for idx in np.arange(1,26):\n",
    "    ax = fig.add_subplot(5, 5, idx, xticks=[], yticks=[])\n",
    "    plt.imshow(images[idx][0,:,:], cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the CNN\n",
    "model = Net()\n",
    "use_cuda=False\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001, weight_decay =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            #clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            #calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            #backward pass: compute gradient of the loss with respect to model paramters\n",
    "            loss.backward()\n",
    "            #perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            #update training loss\n",
    "#             train_loss += loss.item()*data.size(0)\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "    \n",
    "            if batch_idx % 20 == 19:    # print training loss every specified number of mini-batches\n",
    "                print('Epoch %d, Batch %d loss: %.16f' %\n",
    "                      (epoch, batch_idx + 1, train_loss / 20))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            #forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            #calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            #update the validation loss\n",
    "#             valid_loss += loss.item()*data.size(0)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "        #calculate average losses\n",
    "        train_loss_ = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss_ = valid_loss/len(loaders['valid'].dataset)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss_,\n",
    "            valid_loss_\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss_ <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss_))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss_\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 20 loss: 0.0029313538689166\n",
      "Epoch 1, Batch 40 loss: 0.0029163858853281\n",
      "Epoch 1, Batch 60 loss: 0.0028929323889315\n",
      "Epoch 1, Batch 80 loss: 0.0026696848217398\n",
      "Epoch 1, Batch 100 loss: 0.0026091430336237\n",
      "Epoch 1, Batch 120 loss: 0.0026106676086783\n",
      "Epoch 1, Batch 140 loss: 0.0026875957846642\n",
      "Epoch 1, Batch 160 loss: 0.0027570670936257\n",
      "Epoch 1, Batch 180 loss: 0.0027459631673992\n",
      "Epoch 1, Batch 200 loss: 0.0027682287618518\n",
      "Epoch 1, Batch 220 loss: 0.0028255579527467\n",
      "Epoch 1, Batch 240 loss: 0.0029102852568030\n",
      "Epoch 1, Batch 260 loss: 0.0029334968421608\n",
      "Epoch 1, Batch 280 loss: 0.0029942912515253\n",
      "Epoch 1, Batch 300 loss: 0.0031187408603728\n",
      "Epoch 1, Batch 320 loss: 0.0031788672786206\n",
      "Epoch 1, Batch 340 loss: 0.0032331501133740\n",
      "Epoch 1, Batch 360 loss: 0.0032529954332858\n",
      "Epoch 1, Batch 380 loss: 0.0033220401965082\n",
      "Epoch 1, Batch 400 loss: 0.0033901003189385\n",
      "Epoch 1, Batch 420 loss: 0.0035093936603516\n",
      "Epoch 1, Batch 440 loss: 0.0035872862208635\n",
      "Epoch 1, Batch 460 loss: 0.0036805961281061\n",
      "Epoch 1, Batch 480 loss: 0.0037155034951866\n",
      "Epoch 1, Batch 500 loss: 0.0037474532146007\n",
      "Epoch 1, Batch 520 loss: 0.0038169808685780\n",
      "Epoch 1, Batch 540 loss: 0.0038745563942939\n",
      "Epoch 1, Batch 560 loss: 0.0039490829221904\n",
      "Epoch 1, Batch 580 loss: 0.0040146638639271\n",
      "Epoch 1, Batch 600 loss: 0.0040590288117528\n",
      "Epoch 1, Batch 620 loss: 0.0040924842469394\n",
      "Epoch 1, Batch 640 loss: 0.0041376734152436\n",
      "Epoch 1, Batch 660 loss: 0.0041891718283296\n",
      "Epoch 1, Batch 680 loss: 0.0042435214854777\n",
      "Epoch 1, Batch 700 loss: 0.0043174768798053\n",
      "Epoch 1, Batch 720 loss: 0.0043760011903942\n",
      "Epoch 1, Batch 740 loss: 0.0044468538835645\n",
      "Epoch 1, Batch 760 loss: 0.0045068068429828\n",
      "Epoch 1, Batch 780 loss: 0.0045969495549798\n",
      "Epoch: 1 \tTraining Loss: 0.000002 \tValidation Loss: 0.000016\n",
      "Validation loss decreased (inf --> 0.000016).  Saving model ...\n",
      "Epoch 2, Batch 20 loss: 0.0061631230637431\n",
      "Epoch 2, Batch 40 loss: 0.0070967786014080\n",
      "Epoch 2, Batch 60 loss: 0.0070594018325210\n",
      "Epoch 2, Batch 80 loss: 0.0068845772184432\n",
      "Epoch 2, Batch 100 loss: 0.0068243406713009\n",
      "Epoch 2, Batch 120 loss: 0.0069093317724764\n",
      "Epoch 2, Batch 140 loss: 0.0069315643049777\n",
      "Epoch 2, Batch 160 loss: 0.0068124732933939\n",
      "Epoch 2, Batch 180 loss: 0.0068767392076552\n",
      "Epoch 2, Batch 200 loss: 0.0070728845894337\n",
      "Epoch 2, Batch 220 loss: 0.0070650475099683\n",
      "Epoch 2, Batch 240 loss: 0.0070200613699853\n",
      "Epoch 2, Batch 260 loss: 0.0070348619483411\n",
      "Epoch 2, Batch 280 loss: 0.0069464305415750\n",
      "Epoch 2, Batch 300 loss: 0.0069538303650916\n",
      "Epoch 2, Batch 320 loss: 0.0069662434980273\n",
      "Epoch 2, Batch 340 loss: 0.0069813327863812\n",
      "Epoch 2, Batch 360 loss: 0.0070009557530284\n",
      "Epoch 2, Batch 380 loss: 0.0070756315253675\n",
      "Epoch 2, Batch 400 loss: 0.0071278349496424\n",
      "Epoch 2, Batch 420 loss: 0.0071461186744273\n",
      "Epoch 2, Batch 440 loss: 0.0071243555285037\n",
      "Epoch 2, Batch 460 loss: 0.0071577862836421\n",
      "Epoch 2, Batch 480 loss: 0.0071866079233587\n",
      "Epoch 2, Batch 500 loss: 0.0072447946295142\n",
      "Epoch 2, Batch 520 loss: 0.0073011233471334\n",
      "Epoch 2, Batch 540 loss: 0.0073321736417711\n",
      "Epoch 2, Batch 560 loss: 0.0073993662372231\n",
      "Epoch 2, Batch 580 loss: 0.0074262926355004\n",
      "Epoch 2, Batch 600 loss: 0.0074571641162038\n",
      "Epoch 2, Batch 620 loss: 0.0074706450104713\n",
      "Epoch 2, Batch 640 loss: 0.0074869813397527\n",
      "Epoch 2, Batch 660 loss: 0.0075010629370809\n",
      "Epoch 2, Batch 680 loss: 0.0075354194268584\n",
      "Epoch 2, Batch 700 loss: 0.0075306170620024\n",
      "Epoch 2, Batch 720 loss: 0.0075364075601101\n",
      "Epoch 2, Batch 740 loss: 0.0075702047906816\n",
      "Epoch 2, Batch 760 loss: 0.0076023833826184\n",
      "Epoch 2, Batch 780 loss: 0.0076387161388993\n",
      "Epoch: 2 \tTraining Loss: 0.000003 \tValidation Loss: 0.000019\n",
      "Epoch 3, Batch 20 loss: 0.0082391547039151\n",
      "Epoch 3, Batch 40 loss: 0.0079453084617853\n",
      "Epoch 3, Batch 60 loss: 0.0080627398565412\n",
      "Epoch 3, Batch 80 loss: 0.0080796219408512\n",
      "Epoch 3, Batch 100 loss: 0.0079246982932091\n",
      "Epoch 3, Batch 120 loss: 0.0080106761306524\n",
      "Epoch 3, Batch 140 loss: 0.0079537443816662\n",
      "Epoch 3, Batch 160 loss: 0.0080197602510452\n",
      "Epoch 3, Batch 180 loss: 0.0080454926937819\n",
      "Epoch 3, Batch 200 loss: 0.0080103408545256\n",
      "Epoch 3, Batch 220 loss: 0.0079650972038507\n",
      "Epoch 3, Batch 240 loss: 0.0080148512497544\n",
      "Epoch 3, Batch 260 loss: 0.0080495532602072\n",
      "Epoch 3, Batch 280 loss: 0.0080651249736547\n",
      "Epoch 3, Batch 300 loss: 0.0080610802397132\n",
      "Epoch 3, Batch 320 loss: 0.0081574767827988\n",
      "Epoch 3, Batch 340 loss: 0.0082721756771207\n",
      "Epoch 3, Batch 360 loss: 0.0082992706447840\n",
      "Epoch 3, Batch 380 loss: 0.0082546724006534\n",
      "Epoch 3, Batch 400 loss: 0.0082284677773714\n",
      "Epoch 3, Batch 420 loss: 0.0082312989979982\n",
      "Epoch 3, Batch 440 loss: 0.0082150865346193\n",
      "Epoch 3, Batch 460 loss: 0.0081895766779780\n",
      "Epoch 3, Batch 480 loss: 0.0081662284210324\n",
      "Epoch 3, Batch 500 loss: 0.0081736836582422\n",
      "Epoch 3, Batch 520 loss: 0.0081550516188145\n",
      "Epoch 3, Batch 540 loss: 0.0081642437726259\n",
      "Epoch 3, Batch 560 loss: 0.0081799272447824\n",
      "Epoch 3, Batch 580 loss: 0.0081704594194889\n",
      "Epoch 3, Batch 600 loss: 0.0082170804962516\n",
      "Epoch 3, Batch 620 loss: 0.0081902798265219\n",
      "Epoch 3, Batch 640 loss: 0.0081818951293826\n",
      "Epoch 3, Batch 660 loss: 0.0081747947260737\n",
      "Epoch 3, Batch 680 loss: 0.0082566700875759\n",
      "Epoch 3, Batch 700 loss: 0.0082697998732328\n",
      "Epoch 3, Batch 720 loss: 0.0082650957629085\n",
      "Epoch 3, Batch 740 loss: 0.0083065573126078\n",
      "Epoch 3, Batch 760 loss: 0.0083123967051506\n",
      "Epoch 3, Batch 780 loss: 0.0083204535767436\n",
      "Epoch: 3 \tTraining Loss: 0.000003 \tValidation Loss: 0.000019\n",
      "Epoch 4, Batch 20 loss: 0.0082329083234072\n",
      "Epoch 4, Batch 40 loss: 0.0081743430346251\n",
      "Epoch 4, Batch 60 loss: 0.0081898476928473\n",
      "Epoch 4, Batch 80 loss: 0.0082496162503958\n",
      "Epoch 4, Batch 100 loss: 0.0082719950005412\n",
      "Epoch 4, Batch 120 loss: 0.0081842923536897\n",
      "Epoch 4, Batch 140 loss: 0.0081134643405676\n",
      "Epoch 4, Batch 160 loss: 0.0082280281931162\n",
      "Epoch 4, Batch 180 loss: 0.0081552490592003\n",
      "Epoch 4, Batch 200 loss: 0.0080425003543496\n",
      "Epoch 4, Batch 220 loss: 0.0081131272017956\n",
      "Epoch 4, Batch 240 loss: 0.0081290705129504\n",
      "Epoch 4, Batch 260 loss: 0.0081224609166384\n",
      "Epoch 4, Batch 280 loss: 0.0081205759197474\n",
      "Epoch 4, Batch 300 loss: 0.0081352125853300\n",
      "Epoch 4, Batch 320 loss: 0.0080859540030360\n",
      "Epoch 4, Batch 340 loss: 0.0080733671784401\n",
      "Epoch 4, Batch 360 loss: 0.0080884732306004\n",
      "Epoch 4, Batch 380 loss: 0.0080691128969193\n",
      "Epoch 4, Batch 400 loss: 0.0080550927668810\n",
      "Epoch 4, Batch 420 loss: 0.0080686183646321\n",
      "Epoch 4, Batch 440 loss: 0.0080942790955305\n",
      "Epoch 4, Batch 460 loss: 0.0080943657085299\n",
      "Epoch 4, Batch 480 loss: 0.0081454627215862\n",
      "Epoch 4, Batch 500 loss: 0.0081407045945525\n",
      "Epoch 4, Batch 520 loss: 0.0081738978624344\n",
      "Epoch 4, Batch 540 loss: 0.0082139838486910\n",
      "Epoch 4, Batch 560 loss: 0.0082173077389598\n",
      "Epoch 4, Batch 580 loss: 0.0082372175529599\n",
      "Epoch 4, Batch 600 loss: 0.0082414541393518\n",
      "Epoch 4, Batch 620 loss: 0.0082527184858918\n",
      "Epoch 4, Batch 640 loss: 0.0082678887993097\n",
      "Epoch 4, Batch 660 loss: 0.0082504842430353\n",
      "Epoch 4, Batch 680 loss: 0.0082602584734559\n",
      "Epoch 4, Batch 700 loss: 0.0082783093675971\n",
      "Epoch 4, Batch 720 loss: 0.0082829715684056\n",
      "Epoch 4, Batch 740 loss: 0.0082988636568189\n",
      "Epoch 4, Batch 760 loss: 0.0082879392430186\n",
      "Epoch 4, Batch 780 loss: 0.0083054881542921\n",
      "Epoch: 4 \tTraining Loss: 0.000003 \tValidation Loss: 0.000017\n",
      "Epoch 5, Batch 20 loss: 0.0073414347134531\n",
      "Epoch 5, Batch 40 loss: 0.0078131156042218\n",
      "Epoch 5, Batch 60 loss: 0.0076798237860203\n",
      "Epoch 5, Batch 80 loss: 0.0079212952405214\n",
      "Epoch 5, Batch 100 loss: 0.0081056086346507\n",
      "Epoch 5, Batch 120 loss: 0.0080048907548189\n",
      "Epoch 5, Batch 140 loss: 0.0080430088564754\n",
      "Epoch 5, Batch 160 loss: 0.0080610774457455\n",
      "Epoch 5, Batch 180 loss: 0.0081033147871494\n",
      "Epoch 5, Batch 200 loss: 0.0083190333098173\n",
      "Epoch 5, Batch 220 loss: 0.0084371734410524\n",
      "Epoch 5, Batch 240 loss: 0.0083949211984873\n",
      "Epoch 5, Batch 260 loss: 0.0083163417875767\n",
      "Epoch 5, Batch 280 loss: 0.0082255946472287\n",
      "Epoch 5, Batch 300 loss: 0.0083532556891441\n",
      "Epoch 5, Batch 320 loss: 0.0083774356171489\n",
      "Epoch 5, Batch 340 loss: 0.0083723878487945\n",
      "Epoch 5, Batch 360 loss: 0.0083574317395687\n",
      "Epoch 5, Batch 380 loss: 0.0083838189020753\n",
      "Epoch 5, Batch 400 loss: 0.0083698639646173\n",
      "Epoch 5, Batch 420 loss: 0.0083051165565848\n",
      "Epoch 5, Batch 440 loss: 0.0082785319536924\n",
      "Epoch 5, Batch 460 loss: 0.0083196731284261\n",
      "Epoch 5, Batch 480 loss: 0.0083029335364699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 500 loss: 0.0082768332213163\n",
      "Epoch 5, Batch 520 loss: 0.0082514025270939\n",
      "Epoch 5, Batch 540 loss: 0.0082641215994954\n",
      "Epoch 5, Batch 560 loss: 0.0082535594701767\n",
      "Epoch 5, Batch 580 loss: 0.0082643311470747\n",
      "Epoch 5, Batch 600 loss: 0.0082799885421991\n",
      "Epoch 5, Batch 620 loss: 0.0082471864297986\n",
      "Epoch 5, Batch 640 loss: 0.0082471873611212\n",
      "Epoch 5, Batch 660 loss: 0.0082498202100396\n",
      "Epoch 5, Batch 680 loss: 0.0082481391727924\n",
      "Epoch 5, Batch 700 loss: 0.0082372976467013\n",
      "Epoch 5, Batch 720 loss: 0.0082098823040724\n",
      "Epoch 5, Batch 740 loss: 0.0081783151254058\n",
      "Epoch 5, Batch 760 loss: 0.0082055665552616\n",
      "Epoch 5, Batch 780 loss: 0.0081882961094379\n",
      "Epoch: 5 \tTraining Loss: 0.000003 \tValidation Loss: 0.000018\n",
      "Epoch 6, Batch 20 loss: 0.0072178519330919\n",
      "Epoch 6, Batch 40 loss: 0.0072610815986991\n",
      "Epoch 6, Batch 60 loss: 0.0078203957527876\n",
      "Epoch 6, Batch 80 loss: 0.0079558137804270\n",
      "Epoch 6, Batch 100 loss: 0.0080248378217220\n",
      "Epoch 6, Batch 120 loss: 0.0080009745433927\n",
      "Epoch 6, Batch 140 loss: 0.0082402825355530\n",
      "Epoch 6, Batch 160 loss: 0.0083090011030436\n",
      "Epoch 6, Batch 180 loss: 0.0082771312445402\n",
      "Epoch 6, Batch 200 loss: 0.0082785729318857\n",
      "Epoch 6, Batch 220 loss: 0.0082815513014793\n",
      "Epoch 6, Batch 240 loss: 0.0082150157541037\n",
      "Epoch 6, Batch 260 loss: 0.0081337308511138\n",
      "Epoch 6, Batch 280 loss: 0.0081224534660578\n",
      "Epoch 6, Batch 300 loss: 0.0081511745229363\n",
      "Epoch 6, Batch 320 loss: 0.0080429334193468\n",
      "Epoch 6, Batch 340 loss: 0.0080200973898172\n",
      "Epoch 6, Batch 360 loss: 0.0079894717782736\n",
      "Epoch 6, Batch 380 loss: 0.0080287046730518\n",
      "Epoch 6, Batch 400 loss: 0.0080691426992416\n",
      "Epoch 6, Batch 420 loss: 0.0080460365861654\n",
      "Epoch 6, Batch 440 loss: 0.0080528315156698\n",
      "Epoch 6, Batch 460 loss: 0.0080206710845232\n",
      "Epoch 6, Batch 480 loss: 0.0080174384638667\n",
      "Epoch 6, Batch 500 loss: 0.0080081094056368\n",
      "Epoch 6, Batch 520 loss: 0.0080681927502155\n",
      "Epoch 6, Batch 540 loss: 0.0080294841900468\n",
      "Epoch 6, Batch 560 loss: 0.0080649247393012\n",
      "Epoch 6, Batch 580 loss: 0.0080912830308080\n",
      "Epoch 6, Batch 600 loss: 0.0080735404044390\n",
      "Epoch 6, Batch 620 loss: 0.0080779297277331\n",
      "Epoch 6, Batch 640 loss: 0.0080697005614638\n",
      "Epoch 6, Batch 660 loss: 0.0080557931214571\n",
      "Epoch 6, Batch 680 loss: 0.0080749560147524\n",
      "Epoch 6, Batch 700 loss: 0.0080934194847941\n",
      "Epoch 6, Batch 720 loss: 0.0080711198970675\n",
      "Epoch 6, Batch 740 loss: 0.0080617265775800\n",
      "Epoch 6, Batch 760 loss: 0.0080154752358794\n",
      "Epoch 6, Batch 780 loss: 0.0080180512741208\n",
      "Epoch: 6 \tTraining Loss: 0.000003 \tValidation Loss: 0.000017\n",
      "Epoch 7, Batch 20 loss: 0.0087215323001146\n",
      "Epoch 7, Batch 40 loss: 0.0083624543622136\n",
      "Epoch 7, Batch 60 loss: 0.0079307556152344\n",
      "Epoch 7, Batch 80 loss: 0.0078717386350036\n",
      "Epoch 7, Batch 100 loss: 0.0077869235537946\n",
      "Epoch 7, Batch 120 loss: 0.0078862402588129\n",
      "Epoch 7, Batch 140 loss: 0.0079240649938583\n",
      "Epoch 7, Batch 160 loss: 0.0078493999317288\n",
      "Epoch 7, Batch 180 loss: 0.0078835235908628\n",
      "Epoch 7, Batch 200 loss: 0.0078995181247592\n",
      "Epoch 7, Batch 220 loss: 0.0079549290239811\n",
      "Epoch 7, Batch 240 loss: 0.0079783834517002\n",
      "Epoch 7, Batch 260 loss: 0.0079682338982821\n",
      "Epoch 7, Batch 280 loss: 0.0079460125416517\n",
      "Epoch 7, Batch 300 loss: 0.0079213222488761\n",
      "Epoch 7, Batch 320 loss: 0.0079123470932245\n",
      "Epoch 7, Batch 340 loss: 0.0079432558268309\n",
      "Epoch 7, Batch 360 loss: 0.0079445065930486\n",
      "Epoch 7, Batch 380 loss: 0.0079888757318258\n",
      "Epoch 7, Batch 400 loss: 0.0079828174784780\n",
      "Epoch 7, Batch 420 loss: 0.0079675363376737\n",
      "Epoch 7, Batch 440 loss: 0.0079375412315130\n",
      "Epoch 7, Batch 460 loss: 0.0079724956303835\n",
      "Epoch 7, Batch 480 loss: 0.0080580282956362\n",
      "Epoch 7, Batch 500 loss: 0.0080744083970785\n",
      "Epoch 7, Batch 520 loss: 0.0080561460927129\n",
      "Epoch 7, Batch 540 loss: 0.0080474056303501\n",
      "Epoch 7, Batch 560 loss: 0.0080584678798914\n",
      "Epoch 7, Batch 580 loss: 0.0080446079373360\n",
      "Epoch 7, Batch 600 loss: 0.0080084204673767\n",
      "Epoch 7, Batch 620 loss: 0.0080418558791280\n",
      "Epoch 7, Batch 640 loss: 0.0080401999875903\n",
      "Epoch 7, Batch 660 loss: 0.0080171618610620\n",
      "Epoch 7, Batch 680 loss: 0.0080135446041822\n",
      "Epoch 7, Batch 700 loss: 0.0079879155382514\n",
      "Epoch 7, Batch 720 loss: 0.0080050984397531\n",
      "Epoch 7, Batch 740 loss: 0.0079930461943150\n",
      "Epoch 7, Batch 760 loss: 0.0080113373696804\n",
      "Epoch 7, Batch 780 loss: 0.0079982932657003\n",
      "Epoch: 7 \tTraining Loss: 0.000003 \tValidation Loss: 0.000019\n",
      "Epoch 8, Batch 20 loss: 0.0083270324394107\n",
      "Epoch 8, Batch 40 loss: 0.0078743454068899\n",
      "Epoch 8, Batch 60 loss: 0.0077132754959166\n",
      "Epoch 8, Batch 80 loss: 0.0078439358621836\n",
      "Epoch 8, Batch 100 loss: 0.0078449836000800\n",
      "Epoch 8, Batch 120 loss: 0.0077602961100638\n",
      "Epoch 8, Batch 140 loss: 0.0077506443485618\n",
      "Epoch 8, Batch 160 loss: 0.0076708281412721\n",
      "Epoch 8, Batch 180 loss: 0.0077559612691402\n",
      "Epoch 8, Batch 200 loss: 0.0077979033812881\n",
      "Epoch 8, Batch 220 loss: 0.0077742314897478\n",
      "Epoch 8, Batch 240 loss: 0.0077626057900488\n",
      "Epoch 8, Batch 260 loss: 0.0078537492081523\n",
      "Epoch 8, Batch 280 loss: 0.0078695518895984\n",
      "Epoch 8, Batch 300 loss: 0.0078011266887188\n",
      "Epoch 8, Batch 320 loss: 0.0077983774244785\n",
      "Epoch 8, Batch 340 loss: 0.0077369483187795\n",
      "Epoch 8, Batch 360 loss: 0.0077490559779108\n",
      "Epoch 8, Batch 380 loss: 0.0077739609405398\n",
      "Epoch 8, Batch 400 loss: 0.0077600763179362\n",
      "Epoch 8, Batch 420 loss: 0.0077294060029089\n",
      "Epoch 8, Batch 440 loss: 0.0077167153358459\n",
      "Epoch 8, Batch 460 loss: 0.0077484631910920\n",
      "Epoch 8, Batch 480 loss: 0.0077463872730732\n",
      "Epoch 8, Batch 500 loss: 0.0077644041739404\n",
      "Epoch 8, Batch 520 loss: 0.0077556306496263\n",
      "Epoch 8, Batch 540 loss: 0.0077326134778559\n",
      "Epoch 8, Batch 560 loss: 0.0077154198661447\n",
      "Epoch 8, Batch 580 loss: 0.0076946942135692\n",
      "Epoch 8, Batch 600 loss: 0.0076926522888243\n",
      "Epoch 8, Batch 620 loss: 0.0077344207093120\n",
      "Epoch 8, Batch 640 loss: 0.0077885016798973\n",
      "Epoch 8, Batch 660 loss: 0.0078093521296978\n",
      "Epoch 8, Batch 680 loss: 0.0078329229727387\n",
      "Epoch 8, Batch 700 loss: 0.0078365188091993\n",
      "Epoch 8, Batch 720 loss: 0.0078532146289945\n",
      "Epoch 8, Batch 740 loss: 0.0078162634745240\n",
      "Epoch 8, Batch 760 loss: 0.0078315092250705\n",
      "Epoch 8, Batch 780 loss: 0.0078374333679676\n",
      "Epoch: 8 \tTraining Loss: 0.000003 \tValidation Loss: 0.000019\n",
      "Epoch 9, Batch 20 loss: 0.0077872946858406\n",
      "Epoch 9, Batch 40 loss: 0.0080127986148000\n",
      "Epoch 9, Batch 60 loss: 0.0075649777427316\n",
      "Epoch 9, Batch 80 loss: 0.0073867156170309\n",
      "Epoch 9, Batch 100 loss: 0.0074132629670203\n",
      "Epoch 9, Batch 120 loss: 0.0075207157060504\n",
      "Epoch 9, Batch 140 loss: 0.0076325782574713\n",
      "Epoch 9, Batch 160 loss: 0.0076293372549117\n",
      "Epoch 9, Batch 180 loss: 0.0075659314170480\n",
      "Epoch 9, Batch 200 loss: 0.0075778267346323\n",
      "Epoch 9, Batch 220 loss: 0.0075771734118462\n",
      "Epoch 9, Batch 240 loss: 0.0075818509794772\n",
      "Epoch 9, Batch 260 loss: 0.0075808996334672\n",
      "Epoch 9, Batch 280 loss: 0.0075524030253291\n",
      "Epoch 9, Batch 300 loss: 0.0074609206058085\n",
      "Epoch 9, Batch 320 loss: 0.0074203955009580\n",
      "Epoch 9, Batch 340 loss: 0.0074744238518178\n",
      "Epoch 9, Batch 360 loss: 0.0074273133650422\n",
      "Epoch 9, Batch 380 loss: 0.0074172816239297\n",
      "Epoch 9, Batch 400 loss: 0.0074575929902494\n",
      "Epoch 9, Batch 420 loss: 0.0075013590976596\n",
      "Epoch 9, Batch 440 loss: 0.0075825811363757\n",
      "Epoch 9, Batch 460 loss: 0.0076170540414751\n",
      "Epoch 9, Batch 480 loss: 0.0076631763949990\n",
      "Epoch 9, Batch 500 loss: 0.0076950700022280\n",
      "Epoch 9, Batch 520 loss: 0.0077197230421007\n",
      "Epoch 9, Batch 540 loss: 0.0077458843588829\n",
      "Epoch 9, Batch 560 loss: 0.0077094314619899\n",
      "Epoch 9, Batch 580 loss: 0.0077238208614290\n",
      "Epoch 9, Batch 600 loss: 0.0076895570382476\n",
      "Epoch 9, Batch 620 loss: 0.0076980381272733\n",
      "Epoch 9, Batch 640 loss: 0.0077015184797347\n",
      "Epoch 9, Batch 660 loss: 0.0077322898432612\n",
      "Epoch 9, Batch 680 loss: 0.0077453218400478\n",
      "Epoch 9, Batch 700 loss: 0.0077009266242385\n",
      "Epoch 9, Batch 720 loss: 0.0077120414935052\n",
      "Epoch 9, Batch 740 loss: 0.0077123106457293\n",
      "Epoch 9, Batch 760 loss: 0.0077106826938689\n",
      "Epoch 9, Batch 780 loss: 0.0076954825781286\n",
      "Epoch: 9 \tTraining Loss: 0.000003 \tValidation Loss: 0.000018\n",
      "Epoch 10, Batch 20 loss: 0.0075373435392976\n",
      "Epoch 10, Batch 40 loss: 0.0072630732320249\n",
      "Epoch 10, Batch 60 loss: 0.0071667125448585\n",
      "Epoch 10, Batch 80 loss: 0.0071937078610063\n",
      "Epoch 10, Batch 100 loss: 0.0076537830755115\n",
      "Epoch 10, Batch 120 loss: 0.0075222677551210\n",
      "Epoch 10, Batch 140 loss: 0.0074854665435851\n",
      "Epoch 10, Batch 160 loss: 0.0075071081519127\n",
      "Epoch 10, Batch 180 loss: 0.0075259944424033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 200 loss: 0.0076394416391850\n",
      "Epoch 10, Batch 220 loss: 0.0077324742451310\n",
      "Epoch 10, Batch 240 loss: 0.0076592974364758\n",
      "Epoch 10, Batch 260 loss: 0.0076631107367575\n",
      "Epoch 10, Batch 280 loss: 0.0076954611577094\n",
      "Epoch 10, Batch 300 loss: 0.0077062696218491\n",
      "Epoch 10, Batch 320 loss: 0.0076708681881428\n",
      "Epoch 10, Batch 340 loss: 0.0077042751945555\n",
      "Epoch 10, Batch 360 loss: 0.0077602290548384\n",
      "Epoch 10, Batch 380 loss: 0.0077852318063378\n",
      "Epoch 10, Batch 400 loss: 0.0078425165265799\n",
      "Epoch 10, Batch 420 loss: 0.0078135374933481\n",
      "Epoch 10, Batch 440 loss: 0.0077887303195894\n",
      "Epoch 10, Batch 460 loss: 0.0077638877555728\n",
      "Epoch 10, Batch 480 loss: 0.0077408151701093\n",
      "Epoch 10, Batch 500 loss: 0.0077420836314559\n",
      "Epoch 10, Batch 520 loss: 0.0076829204335809\n",
      "Epoch 10, Batch 540 loss: 0.0076396153308451\n",
      "Epoch 10, Batch 560 loss: 0.0076080849394202\n",
      "Epoch 10, Batch 580 loss: 0.0075948201119900\n",
      "Epoch 10, Batch 600 loss: 0.0075827180407941\n",
      "Epoch 10, Batch 620 loss: 0.0075926845893264\n",
      "Epoch 10, Batch 640 loss: 0.0075958808884025\n",
      "Epoch 10, Batch 660 loss: 0.0076375016942620\n",
      "Epoch 10, Batch 680 loss: 0.0076013938523829\n",
      "Epoch 10, Batch 700 loss: 0.0075825424864888\n",
      "Epoch 10, Batch 720 loss: 0.0076332478784025\n",
      "Epoch 10, Batch 740 loss: 0.0076487623155117\n",
      "Epoch 10, Batch 760 loss: 0.0076474049128592\n",
      "Epoch 10, Batch 780 loss: 0.0076537793502212\n",
      "Epoch: 10 \tTraining Loss: 0.000003 \tValidation Loss: 0.000017\n",
      "Epoch 11, Batch 20 loss: 0.0077844872139394\n",
      "Epoch 11, Batch 40 loss: 0.0073758563958108\n",
      "Epoch 11, Batch 60 loss: 0.0079162251204252\n",
      "Epoch 11, Batch 80 loss: 0.0076416274532676\n",
      "Epoch 11, Batch 100 loss: 0.0075236684642732\n",
      "Epoch 11, Batch 120 loss: 0.0076019689440727\n",
      "Epoch 11, Batch 140 loss: 0.0076083401218057\n",
      "Epoch 11, Batch 160 loss: 0.0076704462990165\n",
      "Epoch 11, Batch 180 loss: 0.0075252726674080\n",
      "Epoch 11, Batch 200 loss: 0.0074988780543208\n",
      "Epoch 11, Batch 220 loss: 0.0075256517156959\n",
      "Epoch 11, Batch 240 loss: 0.0076122768223286\n",
      "Epoch 11, Batch 260 loss: 0.0075724138878286\n",
      "Epoch 11, Batch 280 loss: 0.0075393924489617\n",
      "Epoch 11, Batch 300 loss: 0.0076363496482372\n",
      "Epoch 11, Batch 320 loss: 0.0076202973723412\n",
      "Epoch 11, Batch 340 loss: 0.0076487744227052\n",
      "Epoch 11, Batch 360 loss: 0.0076758163049817\n",
      "Epoch 11, Batch 380 loss: 0.0076652104035020\n",
      "Epoch 11, Batch 400 loss: 0.0076033286750317\n",
      "Epoch 11, Batch 420 loss: 0.0076226247474551\n",
      "Epoch 11, Batch 440 loss: 0.0075868302956223\n",
      "Epoch 11, Batch 460 loss: 0.0075870305299759\n",
      "Epoch 11, Batch 480 loss: 0.0075820586644113\n",
      "Epoch 11, Batch 500 loss: 0.0075603239238262\n",
      "Epoch 11, Batch 520 loss: 0.0075650708749890\n",
      "Epoch 11, Batch 540 loss: 0.0075490609742701\n",
      "Epoch 11, Batch 560 loss: 0.0075458176434040\n",
      "Epoch 11, Batch 580 loss: 0.0075323106721044\n",
      "Epoch 11, Batch 600 loss: 0.0074927071109414\n",
      "Epoch 11, Batch 620 loss: 0.0075298426672816\n",
      "Epoch 11, Batch 640 loss: 0.0075516840443015\n",
      "Epoch 11, Batch 660 loss: 0.0075652464292943\n",
      "Epoch 11, Batch 680 loss: 0.0075514288619161\n",
      "Epoch 11, Batch 700 loss: 0.0075538204982877\n",
      "Epoch 11, Batch 720 loss: 0.0075543499551713\n",
      "Epoch 11, Batch 740 loss: 0.0075605302117765\n",
      "Epoch 11, Batch 760 loss: 0.0075927078723907\n",
      "Epoch 11, Batch 780 loss: 0.0075864195823669\n",
      "Epoch: 11 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000016 --> 0.000015).  Saving model ...\n",
      "Epoch 12, Batch 20 loss: 0.0066541656851768\n",
      "Epoch 12, Batch 40 loss: 0.0067898249253631\n",
      "Epoch 12, Batch 60 loss: 0.0066425492987037\n",
      "Epoch 12, Batch 80 loss: 0.0068308738991618\n",
      "Epoch 12, Batch 100 loss: 0.0070100859738886\n",
      "Epoch 12, Batch 120 loss: 0.0071947211399674\n",
      "Epoch 12, Batch 140 loss: 0.0072185182943940\n",
      "Epoch 12, Batch 160 loss: 0.0072685130871832\n",
      "Epoch 12, Batch 180 loss: 0.0074284188449383\n",
      "Epoch 12, Batch 200 loss: 0.0074649131856859\n",
      "Epoch 12, Batch 220 loss: 0.0074738413095474\n",
      "Epoch 12, Batch 240 loss: 0.0073761218227446\n",
      "Epoch 12, Batch 260 loss: 0.0073794079944491\n",
      "Epoch 12, Batch 280 loss: 0.0074311546050012\n",
      "Epoch 12, Batch 300 loss: 0.0074097462929785\n",
      "Epoch 12, Batch 320 loss: 0.0074038975872099\n",
      "Epoch 12, Batch 340 loss: 0.0074071949347854\n",
      "Epoch 12, Batch 360 loss: 0.0074133374728262\n",
      "Epoch 12, Batch 380 loss: 0.0074619250372052\n",
      "Epoch 12, Batch 400 loss: 0.0074550411663949\n",
      "Epoch 12, Batch 420 loss: 0.0073995054699481\n",
      "Epoch 12, Batch 440 loss: 0.0074191121384501\n",
      "Epoch 12, Batch 460 loss: 0.0074110752902925\n",
      "Epoch 12, Batch 480 loss: 0.0074273748323321\n",
      "Epoch 12, Batch 500 loss: 0.0074130455031991\n",
      "Epoch 12, Batch 520 loss: 0.0074193738400936\n",
      "Epoch 12, Batch 540 loss: 0.0074797840788960\n",
      "Epoch 12, Batch 560 loss: 0.0075144395232201\n",
      "Epoch 12, Batch 580 loss: 0.0075224996544421\n",
      "Epoch 12, Batch 600 loss: 0.0075476816855371\n",
      "Epoch 12, Batch 620 loss: 0.0075540416873991\n",
      "Epoch 12, Batch 640 loss: 0.0075611425563693\n",
      "Epoch 12, Batch 660 loss: 0.0075540617108345\n",
      "Epoch 12, Batch 680 loss: 0.0075672240927815\n",
      "Epoch 12, Batch 700 loss: 0.0075839622877538\n",
      "Epoch 12, Batch 720 loss: 0.0075686806812882\n",
      "Epoch 12, Batch 740 loss: 0.0075373924337327\n",
      "Epoch 12, Batch 760 loss: 0.0075698560103774\n",
      "Epoch 12, Batch 780 loss: 0.0076001943089068\n",
      "Epoch: 12 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 13, Batch 20 loss: 0.0077911419793963\n",
      "Epoch 13, Batch 40 loss: 0.0080439513549209\n",
      "Epoch 13, Batch 60 loss: 0.0077093495056033\n",
      "Epoch 13, Batch 80 loss: 0.0075719309970737\n",
      "Epoch 13, Batch 100 loss: 0.0073822960257530\n",
      "Epoch 13, Batch 120 loss: 0.0072245537303388\n",
      "Epoch 13, Batch 140 loss: 0.0071720406413078\n",
      "Epoch 13, Batch 160 loss: 0.0072491196915507\n",
      "Epoch 13, Batch 180 loss: 0.0072053237818182\n",
      "Epoch 13, Batch 200 loss: 0.0071760988794267\n",
      "Epoch 13, Batch 220 loss: 0.0072374246083200\n",
      "Epoch 13, Batch 240 loss: 0.0072170407511294\n",
      "Epoch 13, Batch 260 loss: 0.0072186067700386\n",
      "Epoch 13, Batch 280 loss: 0.0072424737736583\n",
      "Epoch 13, Batch 300 loss: 0.0072244643233716\n",
      "Epoch 13, Batch 320 loss: 0.0072469301521778\n",
      "Epoch 13, Batch 340 loss: 0.0073762172833085\n",
      "Epoch 13, Batch 360 loss: 0.0074173896573484\n",
      "Epoch 13, Batch 380 loss: 0.0074537680484354\n",
      "Epoch 13, Batch 400 loss: 0.0074306344613433\n",
      "Epoch 13, Batch 420 loss: 0.0074099898338318\n",
      "Epoch 13, Batch 440 loss: 0.0074375467374921\n",
      "Epoch 13, Batch 460 loss: 0.0074528306722641\n",
      "Epoch 13, Batch 480 loss: 0.0074146478436887\n",
      "Epoch 13, Batch 500 loss: 0.0074129416607320\n",
      "Epoch 13, Batch 520 loss: 0.0074332552030683\n",
      "Epoch 13, Batch 540 loss: 0.0074570747092366\n",
      "Epoch 13, Batch 560 loss: 0.0074267149902880\n",
      "Epoch 13, Batch 580 loss: 0.0074032219126821\n",
      "Epoch 13, Batch 600 loss: 0.0074245943687856\n",
      "Epoch 13, Batch 620 loss: 0.0074196248315275\n",
      "Epoch 13, Batch 640 loss: 0.0074040032923222\n",
      "Epoch 13, Batch 660 loss: 0.0073877610266209\n",
      "Epoch 13, Batch 680 loss: 0.0073880888521671\n",
      "Epoch 13, Batch 700 loss: 0.0074280588887632\n",
      "Epoch 13, Batch 720 loss: 0.0074357776902616\n",
      "Epoch 13, Batch 740 loss: 0.0074386680498719\n",
      "Epoch 13, Batch 760 loss: 0.0074787447229028\n",
      "Epoch 13, Batch 780 loss: 0.0074944989755750\n",
      "Epoch: 13 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 14, Batch 20 loss: 0.0070175142027438\n",
      "Epoch 14, Batch 40 loss: 0.0072739035822451\n",
      "Epoch 14, Batch 60 loss: 0.0071428804658353\n",
      "Epoch 14, Batch 80 loss: 0.0071947956457734\n",
      "Epoch 14, Batch 100 loss: 0.0071129798889160\n",
      "Epoch 14, Batch 120 loss: 0.0070527507923543\n",
      "Epoch 14, Batch 140 loss: 0.0072311749681830\n",
      "Epoch 14, Batch 160 loss: 0.0073035908862948\n",
      "Epoch 14, Batch 180 loss: 0.0073341676034033\n",
      "Epoch 14, Batch 200 loss: 0.0074570188298821\n",
      "Epoch 14, Batch 220 loss: 0.0074764513410628\n",
      "Epoch 14, Batch 240 loss: 0.0074644424021244\n",
      "Epoch 14, Batch 260 loss: 0.0075110211037099\n",
      "Epoch 14, Batch 280 loss: 0.0075314282439649\n",
      "Epoch 14, Batch 300 loss: 0.0075762150809169\n",
      "Epoch 14, Batch 320 loss: 0.0075421156361699\n",
      "Epoch 14, Batch 340 loss: 0.0074875899590552\n",
      "Epoch 14, Batch 360 loss: 0.0075511671602726\n",
      "Epoch 14, Batch 380 loss: 0.0075535015203059\n",
      "Epoch 14, Batch 400 loss: 0.0075199948623776\n",
      "Epoch 14, Batch 420 loss: 0.0075304717756808\n",
      "Epoch 14, Batch 440 loss: 0.0075150392949581\n",
      "Epoch 14, Batch 460 loss: 0.0074953413568437\n",
      "Epoch 14, Batch 480 loss: 0.0074890106916428\n",
      "Epoch 14, Batch 500 loss: 0.0074411826208234\n",
      "Epoch 14, Batch 520 loss: 0.0074424883350730\n",
      "Epoch 14, Batch 540 loss: 0.0074146725237370\n",
      "Epoch 14, Batch 560 loss: 0.0074170799925923\n",
      "Epoch 14, Batch 580 loss: 0.0074236290529370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 600 loss: 0.0074958316981792\n",
      "Epoch 14, Batch 620 loss: 0.0075122872367501\n",
      "Epoch 14, Batch 640 loss: 0.0074963131919503\n",
      "Epoch 14, Batch 660 loss: 0.0075054154731333\n",
      "Epoch 14, Batch 680 loss: 0.0075285681523383\n",
      "Epoch 14, Batch 700 loss: 0.0075198099948466\n",
      "Epoch 14, Batch 720 loss: 0.0075166737660766\n",
      "Epoch 14, Batch 740 loss: 0.0075304070487618\n",
      "Epoch 14, Batch 760 loss: 0.0075122080743313\n",
      "Epoch 14, Batch 780 loss: 0.0074952617287636\n",
      "Epoch: 14 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000015 --> 0.000015).  Saving model ...\n",
      "Epoch 15, Batch 20 loss: 0.0070587312802672\n",
      "Epoch 15, Batch 40 loss: 0.0069287358783185\n",
      "Epoch 15, Batch 60 loss: 0.0068533411249518\n",
      "Epoch 15, Batch 80 loss: 0.0068918904289603\n",
      "Epoch 15, Batch 100 loss: 0.0070513086393476\n",
      "Epoch 15, Batch 120 loss: 0.0070975855924189\n",
      "Epoch 15, Batch 140 loss: 0.0072043561376631\n",
      "Epoch 15, Batch 160 loss: 0.0071910666301847\n",
      "Epoch 15, Batch 180 loss: 0.0071350261569023\n",
      "Epoch 15, Batch 200 loss: 0.0072363028302789\n",
      "Epoch 15, Batch 220 loss: 0.0071928324177861\n",
      "Epoch 15, Batch 240 loss: 0.0071272589266300\n",
      "Epoch 15, Batch 260 loss: 0.0071160360239446\n",
      "Epoch 15, Batch 280 loss: 0.0071741575375199\n",
      "Epoch 15, Batch 300 loss: 0.0071857511065900\n",
      "Epoch 15, Batch 320 loss: 0.0072051836177707\n",
      "Epoch 15, Batch 340 loss: 0.0072866068221629\n",
      "Epoch 15, Batch 360 loss: 0.0073120831511915\n",
      "Epoch 15, Batch 380 loss: 0.0072646541520953\n",
      "Epoch 15, Batch 400 loss: 0.0072740889154375\n",
      "Epoch 15, Batch 420 loss: 0.0072961421683431\n",
      "Epoch 15, Batch 440 loss: 0.0073339142836630\n",
      "Epoch 15, Batch 460 loss: 0.0073385192081332\n",
      "Epoch 15, Batch 480 loss: 0.0073619224131107\n",
      "Epoch 15, Batch 500 loss: 0.0073655410669744\n",
      "Epoch 15, Batch 520 loss: 0.0073669007979333\n",
      "Epoch 15, Batch 540 loss: 0.0073875016532838\n",
      "Epoch 15, Batch 560 loss: 0.0074318447150290\n",
      "Epoch 15, Batch 580 loss: 0.0074442857876420\n",
      "Epoch 15, Batch 600 loss: 0.0074172094464302\n",
      "Epoch 15, Batch 620 loss: 0.0074404301121831\n",
      "Epoch 15, Batch 640 loss: 0.0075027048587799\n",
      "Epoch 15, Batch 660 loss: 0.0075009702704847\n",
      "Epoch 15, Batch 680 loss: 0.0074934167787433\n",
      "Epoch 15, Batch 700 loss: 0.0075119510293007\n",
      "Epoch 15, Batch 720 loss: 0.0075246123597026\n",
      "Epoch 15, Batch 740 loss: 0.0075023979879916\n",
      "Epoch 15, Batch 760 loss: 0.0074850125238299\n",
      "Epoch 15, Batch 780 loss: 0.0074773887172341\n",
      "Epoch: 15 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 16, Batch 20 loss: 0.0069889225997031\n",
      "Epoch 16, Batch 40 loss: 0.0073749683797359\n",
      "Epoch 16, Batch 60 loss: 0.0073770657181740\n",
      "Epoch 16, Batch 80 loss: 0.0075785205699503\n",
      "Epoch 16, Batch 100 loss: 0.0076366267167032\n",
      "Epoch 16, Batch 120 loss: 0.0074155428446829\n",
      "Epoch 16, Batch 140 loss: 0.0073506785556674\n",
      "Epoch 16, Batch 160 loss: 0.0072765811346471\n",
      "Epoch 16, Batch 180 loss: 0.0072494437918067\n",
      "Epoch 16, Batch 200 loss: 0.0073140994645655\n",
      "Epoch 16, Batch 220 loss: 0.0073254751041532\n",
      "Epoch 16, Batch 240 loss: 0.0072834887541831\n",
      "Epoch 16, Batch 260 loss: 0.0072191916406155\n",
      "Epoch 16, Batch 280 loss: 0.0071699628606439\n",
      "Epoch 16, Batch 300 loss: 0.0071508949622512\n",
      "Epoch 16, Batch 320 loss: 0.0071846470236778\n",
      "Epoch 16, Batch 340 loss: 0.0071659213863313\n",
      "Epoch 16, Batch 360 loss: 0.0071953190490603\n",
      "Epoch 16, Batch 380 loss: 0.0072411410510540\n",
      "Epoch 16, Batch 400 loss: 0.0072377920150757\n",
      "Epoch 16, Batch 420 loss: 0.0072483858093619\n",
      "Epoch 16, Batch 440 loss: 0.0072642304003239\n",
      "Epoch 16, Batch 460 loss: 0.0072988183237612\n",
      "Epoch 16, Batch 480 loss: 0.0072862878441811\n",
      "Epoch 16, Batch 500 loss: 0.0072943442501128\n",
      "Epoch 16, Batch 520 loss: 0.0073104901239276\n",
      "Epoch 16, Batch 540 loss: 0.0073119229637086\n",
      "Epoch 16, Batch 560 loss: 0.0073232115246356\n",
      "Epoch 16, Batch 580 loss: 0.0073144533671439\n",
      "Epoch 16, Batch 600 loss: 0.0073092579841614\n",
      "Epoch 16, Batch 620 loss: 0.0073151378892362\n",
      "Epoch 16, Batch 640 loss: 0.0073426431044936\n",
      "Epoch 16, Batch 660 loss: 0.0073408214375377\n",
      "Epoch 16, Batch 680 loss: 0.0073425462469459\n",
      "Epoch 16, Batch 700 loss: 0.0073437122628093\n",
      "Epoch 16, Batch 720 loss: 0.0073542566969991\n",
      "Epoch 16, Batch 740 loss: 0.0073657133616507\n",
      "Epoch 16, Batch 760 loss: 0.0073893130756915\n",
      "Epoch 16, Batch 780 loss: 0.0073890006169677\n",
      "Epoch: 16 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 17, Batch 20 loss: 0.0074654323980212\n",
      "Epoch 17, Batch 40 loss: 0.0068287849426270\n",
      "Epoch 17, Batch 60 loss: 0.0068906149826944\n",
      "Epoch 17, Batch 80 loss: 0.0068722837604582\n",
      "Epoch 17, Batch 100 loss: 0.0069554178044200\n",
      "Epoch 17, Batch 120 loss: 0.0069988719187677\n",
      "Epoch 17, Batch 140 loss: 0.0071496209129691\n",
      "Epoch 17, Batch 160 loss: 0.0070736580528319\n",
      "Epoch 17, Batch 180 loss: 0.0069945878349245\n",
      "Epoch 17, Batch 200 loss: 0.0070670871064067\n",
      "Epoch 17, Batch 220 loss: 0.0070652649737895\n",
      "Epoch 17, Batch 240 loss: 0.0070206848904490\n",
      "Epoch 17, Batch 260 loss: 0.0070610134862363\n",
      "Epoch 17, Batch 280 loss: 0.0071029029786587\n",
      "Epoch 17, Batch 300 loss: 0.0071523874066770\n",
      "Epoch 17, Batch 320 loss: 0.0072140851989388\n",
      "Epoch 17, Batch 340 loss: 0.0072696148417890\n",
      "Epoch 17, Batch 360 loss: 0.0073327841237187\n",
      "Epoch 17, Batch 380 loss: 0.0073196617886424\n",
      "Epoch 17, Batch 400 loss: 0.0073303543031216\n",
      "Epoch 17, Batch 420 loss: 0.0073101655580103\n",
      "Epoch 17, Batch 440 loss: 0.0073381261900067\n",
      "Epoch 17, Batch 460 loss: 0.0073310034349561\n",
      "Epoch 17, Batch 480 loss: 0.0073065357282758\n",
      "Epoch 17, Batch 500 loss: 0.0073003238067031\n",
      "Epoch 17, Batch 520 loss: 0.0073091299273074\n",
      "Epoch 17, Batch 540 loss: 0.0072992392815650\n",
      "Epoch 17, Batch 560 loss: 0.0072900070808828\n",
      "Epoch 17, Batch 580 loss: 0.0072686425410211\n",
      "Epoch 17, Batch 600 loss: 0.0072934292256832\n",
      "Epoch 17, Batch 620 loss: 0.0073258774355054\n",
      "Epoch 17, Batch 640 loss: 0.0072983442805707\n",
      "Epoch 17, Batch 660 loss: 0.0072839283384383\n",
      "Epoch 17, Batch 680 loss: 0.0073135369457304\n",
      "Epoch 17, Batch 700 loss: 0.0073488997295499\n",
      "Epoch 17, Batch 720 loss: 0.0073842396959662\n",
      "Epoch 17, Batch 740 loss: 0.0074179768562317\n",
      "Epoch 17, Batch 760 loss: 0.0074093476869166\n",
      "Epoch 17, Batch 780 loss: 0.0074152648448944\n",
      "Epoch: 17 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000015 --> 0.000015).  Saving model ...\n",
      "Epoch 18, Batch 20 loss: 0.0073340632952750\n",
      "Epoch 18, Batch 40 loss: 0.0069056199863553\n",
      "Epoch 18, Batch 60 loss: 0.0069488845765591\n",
      "Epoch 18, Batch 80 loss: 0.0071476427838206\n",
      "Epoch 18, Batch 100 loss: 0.0069781178608537\n",
      "Epoch 18, Batch 120 loss: 0.0071987211704254\n",
      "Epoch 18, Batch 140 loss: 0.0072198309935629\n",
      "Epoch 18, Batch 160 loss: 0.0071515580639243\n",
      "Epoch 18, Batch 180 loss: 0.0070980181917548\n",
      "Epoch 18, Batch 200 loss: 0.0070832245983183\n",
      "Epoch 18, Batch 220 loss: 0.0071586510166526\n",
      "Epoch 18, Batch 240 loss: 0.0071654939092696\n",
      "Epoch 18, Batch 260 loss: 0.0072433813475072\n",
      "Epoch 18, Batch 280 loss: 0.0072272033430636\n",
      "Epoch 18, Batch 300 loss: 0.0072730360552669\n",
      "Epoch 18, Batch 320 loss: 0.0072849476709962\n",
      "Epoch 18, Batch 340 loss: 0.0072866557165980\n",
      "Epoch 18, Batch 360 loss: 0.0073369210585952\n",
      "Epoch 18, Batch 380 loss: 0.0072693205438554\n",
      "Epoch 18, Batch 400 loss: 0.0072647505439818\n",
      "Epoch 18, Batch 420 loss: 0.0072681335732341\n",
      "Epoch 18, Batch 440 loss: 0.0072768377140164\n",
      "Epoch 18, Batch 460 loss: 0.0073174037970603\n",
      "Epoch 18, Batch 480 loss: 0.0073372484184802\n",
      "Epoch 18, Batch 500 loss: 0.0073513113893569\n",
      "Epoch 18, Batch 520 loss: 0.0073749544098973\n",
      "Epoch 18, Batch 540 loss: 0.0074052521958947\n",
      "Epoch 18, Batch 560 loss: 0.0074086599051952\n",
      "Epoch 18, Batch 580 loss: 0.0073918150737882\n",
      "Epoch 18, Batch 600 loss: 0.0073692114092410\n",
      "Epoch 18, Batch 620 loss: 0.0073740920051932\n",
      "Epoch 18, Batch 640 loss: 0.0073556071147323\n",
      "Epoch 18, Batch 660 loss: 0.0073533123359084\n",
      "Epoch 18, Batch 680 loss: 0.0073630935512483\n",
      "Epoch 18, Batch 700 loss: 0.0073605948127806\n",
      "Epoch 18, Batch 720 loss: 0.0073448447510600\n",
      "Epoch 18, Batch 740 loss: 0.0073540373705328\n",
      "Epoch 18, Batch 760 loss: 0.0073421322740614\n",
      "Epoch 18, Batch 780 loss: 0.0073386942967772\n",
      "Epoch: 18 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 19, Batch 20 loss: 0.0080419052392244\n",
      "Epoch 19, Batch 40 loss: 0.0074146860279143\n",
      "Epoch 19, Batch 60 loss: 0.0071310503408313\n",
      "Epoch 19, Batch 80 loss: 0.0073013030923903\n",
      "Epoch 19, Batch 100 loss: 0.0074088959954679\n",
      "Epoch 19, Batch 120 loss: 0.0073635727167130\n",
      "Epoch 19, Batch 140 loss: 0.0073635340668261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 160 loss: 0.0073524392209947\n",
      "Epoch 19, Batch 180 loss: 0.0073425038717687\n",
      "Epoch 19, Batch 200 loss: 0.0073069347999990\n",
      "Epoch 19, Batch 220 loss: 0.0073057436384261\n",
      "Epoch 19, Batch 240 loss: 0.0073791914619505\n",
      "Epoch 19, Batch 260 loss: 0.0073962532915175\n",
      "Epoch 19, Batch 280 loss: 0.0073877042159438\n",
      "Epoch 19, Batch 300 loss: 0.0074208751320839\n",
      "Epoch 19, Batch 320 loss: 0.0074778860434890\n",
      "Epoch 19, Batch 340 loss: 0.0075214742682874\n",
      "Epoch 19, Batch 360 loss: 0.0074630221351981\n",
      "Epoch 19, Batch 380 loss: 0.0074399970471859\n",
      "Epoch 19, Batch 400 loss: 0.0073531894013286\n",
      "Epoch 19, Batch 420 loss: 0.0073678037151694\n",
      "Epoch 19, Batch 440 loss: 0.0073758857324719\n",
      "Epoch 19, Batch 460 loss: 0.0073305754922330\n",
      "Epoch 19, Batch 480 loss: 0.0074036694131792\n",
      "Epoch 19, Batch 500 loss: 0.0074419295415282\n",
      "Epoch 19, Batch 520 loss: 0.0074298218823969\n",
      "Epoch 19, Batch 540 loss: 0.0074181230738759\n",
      "Epoch 19, Batch 560 loss: 0.0074160308577120\n",
      "Epoch 19, Batch 580 loss: 0.0074030742980540\n",
      "Epoch 19, Batch 600 loss: 0.0073723248206079\n",
      "Epoch 19, Batch 620 loss: 0.0073951585218310\n",
      "Epoch 19, Batch 640 loss: 0.0074042864143848\n",
      "Epoch 19, Batch 660 loss: 0.0074201426468790\n",
      "Epoch 19, Batch 680 loss: 0.0074411495588720\n",
      "Epoch 19, Batch 700 loss: 0.0074641094543040\n",
      "Epoch 19, Batch 720 loss: 0.0074559524655342\n",
      "Epoch 19, Batch 740 loss: 0.0074221142567694\n",
      "Epoch 19, Batch 760 loss: 0.0074581899680197\n",
      "Epoch 19, Batch 780 loss: 0.0074575529433787\n",
      "Epoch: 19 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 20, Batch 20 loss: 0.0082875955849886\n",
      "Epoch 20, Batch 40 loss: 0.0076772728934884\n",
      "Epoch 20, Batch 60 loss: 0.0073062740266323\n",
      "Epoch 20, Batch 80 loss: 0.0072029284201562\n",
      "Epoch 20, Batch 100 loss: 0.0071007395163178\n",
      "Epoch 20, Batch 120 loss: 0.0070724054239690\n",
      "Epoch 20, Batch 140 loss: 0.0071376571431756\n",
      "Epoch 20, Batch 160 loss: 0.0071975914761424\n",
      "Epoch 20, Batch 180 loss: 0.0071164555847645\n",
      "Epoch 20, Batch 200 loss: 0.0071895569562912\n",
      "Epoch 20, Batch 220 loss: 0.0072121708653867\n",
      "Epoch 20, Batch 240 loss: 0.0072771548293531\n",
      "Epoch 20, Batch 260 loss: 0.0073293000459671\n",
      "Epoch 20, Batch 280 loss: 0.0072690704837441\n",
      "Epoch 20, Batch 300 loss: 0.0071975351311266\n",
      "Epoch 20, Batch 320 loss: 0.0071836998686194\n",
      "Epoch 20, Batch 340 loss: 0.0071782628074288\n",
      "Epoch 20, Batch 360 loss: 0.0071672713384032\n",
      "Epoch 20, Batch 380 loss: 0.0071937292814255\n",
      "Epoch 20, Batch 400 loss: 0.0071962410584092\n",
      "Epoch 20, Batch 420 loss: 0.0071636005304754\n",
      "Epoch 20, Batch 440 loss: 0.0071702720597386\n",
      "Epoch 20, Batch 460 loss: 0.0072079068049788\n",
      "Epoch 20, Batch 480 loss: 0.0072260485030711\n",
      "Epoch 20, Batch 500 loss: 0.0072570005431771\n",
      "Epoch 20, Batch 520 loss: 0.0072635719552636\n",
      "Epoch 20, Batch 540 loss: 0.0072853332385421\n",
      "Epoch 20, Batch 560 loss: 0.0072669619694352\n",
      "Epoch 20, Batch 580 loss: 0.0072574326768517\n",
      "Epoch 20, Batch 600 loss: 0.0072519006207585\n",
      "Epoch 20, Batch 620 loss: 0.0072341100312769\n",
      "Epoch 20, Batch 640 loss: 0.0072365612722933\n",
      "Epoch 20, Batch 660 loss: 0.0072669312357903\n",
      "Epoch 20, Batch 680 loss: 0.0072624869644642\n",
      "Epoch 20, Batch 700 loss: 0.0072610192000866\n",
      "Epoch 20, Batch 720 loss: 0.0072754384018481\n",
      "Epoch 20, Batch 740 loss: 0.0072803534567356\n",
      "Epoch 20, Batch 760 loss: 0.0072901109233499\n",
      "Epoch 20, Batch 780 loss: 0.0072822696529329\n",
      "Epoch: 20 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 21, Batch 20 loss: 0.0080835530534387\n",
      "Epoch 21, Batch 40 loss: 0.0076710879802704\n",
      "Epoch 21, Batch 60 loss: 0.0073919654823840\n",
      "Epoch 21, Batch 80 loss: 0.0071538812480867\n",
      "Epoch 21, Batch 100 loss: 0.0070781810209155\n",
      "Epoch 21, Batch 120 loss: 0.0070338747464120\n",
      "Epoch 21, Batch 140 loss: 0.0070543242618442\n",
      "Epoch 21, Batch 160 loss: 0.0070064589381218\n",
      "Epoch 21, Batch 180 loss: 0.0069955750368536\n",
      "Epoch 21, Batch 200 loss: 0.0071213170886040\n",
      "Epoch 21, Batch 220 loss: 0.0071365162730217\n",
      "Epoch 21, Batch 240 loss: 0.0071523236110806\n",
      "Epoch 21, Batch 260 loss: 0.0070767565630376\n",
      "Epoch 21, Batch 280 loss: 0.0070748529396951\n",
      "Epoch 21, Batch 300 loss: 0.0071386015042663\n",
      "Epoch 21, Batch 320 loss: 0.0070893666706979\n",
      "Epoch 21, Batch 340 loss: 0.0070410952903330\n",
      "Epoch 21, Batch 360 loss: 0.0071405707858503\n",
      "Epoch 21, Batch 380 loss: 0.0071070166304708\n",
      "Epoch 21, Batch 400 loss: 0.0071380501613021\n",
      "Epoch 21, Batch 420 loss: 0.0071318512782454\n",
      "Epoch 21, Batch 440 loss: 0.0071489019319415\n",
      "Epoch 21, Batch 460 loss: 0.0072126472368836\n",
      "Epoch 21, Batch 480 loss: 0.0072210235521197\n",
      "Epoch 21, Batch 500 loss: 0.0071738050319254\n",
      "Epoch 21, Batch 520 loss: 0.0071763419546187\n",
      "Epoch 21, Batch 540 loss: 0.0071692569181323\n",
      "Epoch 21, Batch 560 loss: 0.0071960287168622\n",
      "Epoch 21, Batch 580 loss: 0.0072347046807408\n",
      "Epoch 21, Batch 600 loss: 0.0072584724985063\n",
      "Epoch 21, Batch 620 loss: 0.0073020765557885\n",
      "Epoch 21, Batch 640 loss: 0.0073128687217832\n",
      "Epoch 21, Batch 660 loss: 0.0072760828770697\n",
      "Epoch 21, Batch 680 loss: 0.0072926888242364\n",
      "Epoch 21, Batch 700 loss: 0.0072886096313596\n",
      "Epoch 21, Batch 720 loss: 0.0073040239512920\n",
      "Epoch 21, Batch 740 loss: 0.0073274979367852\n",
      "Epoch 21, Batch 760 loss: 0.0073131434619427\n",
      "Epoch 21, Batch 780 loss: 0.0073316395282745\n",
      "Epoch: 21 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 22, Batch 20 loss: 0.0057621598243713\n",
      "Epoch 22, Batch 40 loss: 0.0061746453866363\n",
      "Epoch 22, Batch 60 loss: 0.0065584196709096\n",
      "Epoch 22, Batch 80 loss: 0.0067611634731293\n",
      "Epoch 22, Batch 100 loss: 0.0067628510296345\n",
      "Epoch 22, Batch 120 loss: 0.0068484367802739\n",
      "Epoch 22, Batch 140 loss: 0.0066785300150514\n",
      "Epoch 22, Batch 160 loss: 0.0065993024036288\n",
      "Epoch 22, Batch 180 loss: 0.0066831251606345\n",
      "Epoch 22, Batch 200 loss: 0.0067859417758882\n",
      "Epoch 22, Batch 220 loss: 0.0069005107507110\n",
      "Epoch 22, Batch 240 loss: 0.0069479895755649\n",
      "Epoch 22, Batch 260 loss: 0.0070365928113461\n",
      "Epoch 22, Batch 280 loss: 0.0071383686736226\n",
      "Epoch 22, Batch 300 loss: 0.0071884007193148\n",
      "Epoch 22, Batch 320 loss: 0.0071842237375677\n",
      "Epoch 22, Batch 340 loss: 0.0072640529833734\n",
      "Epoch 22, Batch 360 loss: 0.0072819292545319\n",
      "Epoch 22, Batch 380 loss: 0.0072539127431810\n",
      "Epoch 22, Batch 400 loss: 0.0072115994989872\n",
      "Epoch 22, Batch 420 loss: 0.0072032124735415\n",
      "Epoch 22, Batch 440 loss: 0.0072082802653313\n",
      "Epoch 22, Batch 460 loss: 0.0072433901950717\n",
      "Epoch 22, Batch 480 loss: 0.0072308154776692\n",
      "Epoch 22, Batch 500 loss: 0.0072650783695281\n",
      "Epoch 22, Batch 520 loss: 0.0072648837231100\n",
      "Epoch 22, Batch 540 loss: 0.0072047477588058\n",
      "Epoch 22, Batch 560 loss: 0.0072083151899278\n",
      "Epoch 22, Batch 580 loss: 0.0072007365524769\n",
      "Epoch 22, Batch 600 loss: 0.0072125657461584\n",
      "Epoch 22, Batch 620 loss: 0.0072023398242891\n",
      "Epoch 22, Batch 640 loss: 0.0071992813609540\n",
      "Epoch 22, Batch 660 loss: 0.0071874507702887\n",
      "Epoch 22, Batch 680 loss: 0.0072170607745647\n",
      "Epoch 22, Batch 700 loss: 0.0071925781667233\n",
      "Epoch 22, Batch 720 loss: 0.0072079063393176\n",
      "Epoch 22, Batch 740 loss: 0.0072201997973025\n",
      "Epoch 22, Batch 760 loss: 0.0071915248408914\n",
      "Epoch 22, Batch 780 loss: 0.0071987872943282\n",
      "Epoch: 22 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 23, Batch 20 loss: 0.0077098719775677\n",
      "Epoch 23, Batch 40 loss: 0.0074895760044456\n",
      "Epoch 23, Batch 60 loss: 0.0071781636215746\n",
      "Epoch 23, Batch 80 loss: 0.0074248486198485\n",
      "Epoch 23, Batch 100 loss: 0.0072076194919646\n",
      "Epoch 23, Batch 120 loss: 0.0073308893479407\n",
      "Epoch 23, Batch 140 loss: 0.0071834078989923\n",
      "Epoch 23, Batch 160 loss: 0.0070226513780653\n",
      "Epoch 23, Batch 180 loss: 0.0071260160766542\n",
      "Epoch 23, Batch 200 loss: 0.0071987411938608\n",
      "Epoch 23, Batch 220 loss: 0.0071698455139995\n",
      "Epoch 23, Batch 240 loss: 0.0071824761107564\n",
      "Epoch 23, Batch 260 loss: 0.0071973567828536\n",
      "Epoch 23, Batch 280 loss: 0.0072865015827119\n",
      "Epoch 23, Batch 300 loss: 0.0073343021795154\n",
      "Epoch 23, Batch 320 loss: 0.0073226517997682\n",
      "Epoch 23, Batch 340 loss: 0.0072969445027411\n",
      "Epoch 23, Batch 360 loss: 0.0073178834281862\n",
      "Epoch 23, Batch 380 loss: 0.0073474189266562\n",
      "Epoch 23, Batch 400 loss: 0.0073485248722136\n",
      "Epoch 23, Batch 420 loss: 0.0073861675336957\n",
      "Epoch 23, Batch 440 loss: 0.0074024396017194\n",
      "Epoch 23, Batch 460 loss: 0.0073743076063693\n",
      "Epoch 23, Batch 480 loss: 0.0073073417879641\n",
      "Epoch 23, Batch 500 loss: 0.0072760330513120\n",
      "Epoch 23, Batch 520 loss: 0.0072987438179553\n",
      "Epoch 23, Batch 540 loss: 0.0073110335506499\n",
      "Epoch 23, Batch 560 loss: 0.0073305545374751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 580 loss: 0.0073007405735552\n",
      "Epoch 23, Batch 600 loss: 0.0073013552464545\n",
      "Epoch 23, Batch 620 loss: 0.0073241801001132\n",
      "Epoch 23, Batch 640 loss: 0.0073440792039037\n",
      "Epoch 23, Batch 660 loss: 0.0073364204727113\n",
      "Epoch 23, Batch 680 loss: 0.0073129669763148\n",
      "Epoch 23, Batch 700 loss: 0.0073027960024774\n",
      "Epoch 23, Batch 720 loss: 0.0072926930151880\n",
      "Epoch 23, Batch 740 loss: 0.0073085604235530\n",
      "Epoch 23, Batch 760 loss: 0.0073018036782742\n",
      "Epoch 23, Batch 780 loss: 0.0072933919727802\n",
      "Epoch: 23 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000015 --> 0.000015).  Saving model ...\n",
      "Epoch 24, Batch 20 loss: 0.0068174102343619\n",
      "Epoch 24, Batch 40 loss: 0.0068896063603461\n",
      "Epoch 24, Batch 60 loss: 0.0067790532484651\n",
      "Epoch 24, Batch 80 loss: 0.0069411955773830\n",
      "Epoch 24, Batch 100 loss: 0.0070849410258234\n",
      "Epoch 24, Batch 120 loss: 0.0071457787416875\n",
      "Epoch 24, Batch 140 loss: 0.0071343267336488\n",
      "Epoch 24, Batch 160 loss: 0.0071725794114172\n",
      "Epoch 24, Batch 180 loss: 0.0071793594397604\n",
      "Epoch 24, Batch 200 loss: 0.0071809738874435\n",
      "Epoch 24, Batch 220 loss: 0.0072225825861096\n",
      "Epoch 24, Batch 240 loss: 0.0073100114241242\n",
      "Epoch 24, Batch 260 loss: 0.0072732744738460\n",
      "Epoch 24, Batch 280 loss: 0.0072988821193576\n",
      "Epoch 24, Batch 300 loss: 0.0072435168549418\n",
      "Epoch 24, Batch 320 loss: 0.0072061540558934\n",
      "Epoch 24, Batch 340 loss: 0.0072017125785351\n",
      "Epoch 24, Batch 360 loss: 0.0071554020978510\n",
      "Epoch 24, Batch 380 loss: 0.0071187042631209\n",
      "Epoch 24, Batch 400 loss: 0.0070738056674600\n",
      "Epoch 24, Batch 420 loss: 0.0070415278896689\n",
      "Epoch 24, Batch 440 loss: 0.0070609683170915\n",
      "Epoch 24, Batch 460 loss: 0.0071128471754491\n",
      "Epoch 24, Batch 480 loss: 0.0070872744545341\n",
      "Epoch 24, Batch 500 loss: 0.0070957965217531\n",
      "Epoch 24, Batch 520 loss: 0.0071172653697431\n",
      "Epoch 24, Batch 540 loss: 0.0071303732693195\n",
      "Epoch 24, Batch 560 loss: 0.0071563697420061\n",
      "Epoch 24, Batch 580 loss: 0.0071462327614427\n",
      "Epoch 24, Batch 600 loss: 0.0071083344519138\n",
      "Epoch 24, Batch 620 loss: 0.0071293623186648\n",
      "Epoch 24, Batch 640 loss: 0.0071624135598540\n",
      "Epoch 24, Batch 660 loss: 0.0071752793155611\n",
      "Epoch 24, Batch 680 loss: 0.0072070145979524\n",
      "Epoch 24, Batch 700 loss: 0.0072324164211750\n",
      "Epoch 24, Batch 720 loss: 0.0072404993698001\n",
      "Epoch 24, Batch 740 loss: 0.0072797238826752\n",
      "Epoch 24, Batch 760 loss: 0.0072539499960840\n",
      "Epoch 24, Batch 780 loss: 0.0072490172460675\n",
      "Epoch: 24 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 25, Batch 20 loss: 0.0072854002937675\n",
      "Epoch 25, Batch 40 loss: 0.0071340166032314\n",
      "Epoch 25, Batch 60 loss: 0.0071138218045235\n",
      "Epoch 25, Batch 80 loss: 0.0073143541812897\n",
      "Epoch 25, Batch 100 loss: 0.0072663798928261\n",
      "Epoch 25, Batch 120 loss: 0.0074317618273199\n",
      "Epoch 25, Batch 140 loss: 0.0073321894742548\n",
      "Epoch 25, Batch 160 loss: 0.0071670538745821\n",
      "Epoch 25, Batch 180 loss: 0.0070770457386971\n",
      "Epoch 25, Batch 200 loss: 0.0071263252757490\n",
      "Epoch 25, Batch 220 loss: 0.0071083842776716\n",
      "Epoch 25, Batch 240 loss: 0.0071685845032334\n",
      "Epoch 25, Batch 260 loss: 0.0071244426071644\n",
      "Epoch 25, Batch 280 loss: 0.0070380689576268\n",
      "Epoch 25, Batch 300 loss: 0.0070196748711169\n",
      "Epoch 25, Batch 320 loss: 0.0070168264210224\n",
      "Epoch 25, Batch 340 loss: 0.0069447620771825\n",
      "Epoch 25, Batch 360 loss: 0.0069302581250668\n",
      "Epoch 25, Batch 380 loss: 0.0069454610347748\n",
      "Epoch 25, Batch 400 loss: 0.0070148156955838\n",
      "Epoch 25, Batch 420 loss: 0.0070485211908817\n",
      "Epoch 25, Batch 440 loss: 0.0070709986612201\n",
      "Epoch 25, Batch 460 loss: 0.0071168765425682\n",
      "Epoch 25, Batch 480 loss: 0.0071135326288640\n",
      "Epoch 25, Batch 500 loss: 0.0070958444848657\n",
      "Epoch 25, Batch 520 loss: 0.0070891878567636\n",
      "Epoch 25, Batch 540 loss: 0.0070925243198872\n",
      "Epoch 25, Batch 560 loss: 0.0071018002927303\n",
      "Epoch 25, Batch 580 loss: 0.0071109971031547\n",
      "Epoch 25, Batch 600 loss: 0.0071333372034132\n",
      "Epoch 25, Batch 620 loss: 0.0071822092868388\n",
      "Epoch 25, Batch 640 loss: 0.0071830041706562\n",
      "Epoch 25, Batch 660 loss: 0.0071837110444903\n",
      "Epoch 25, Batch 680 loss: 0.0072193765081465\n",
      "Epoch 25, Batch 700 loss: 0.0072081289254129\n",
      "Epoch 25, Batch 720 loss: 0.0072376793250442\n",
      "Epoch 25, Batch 740 loss: 0.0072266533970833\n",
      "Epoch 25, Batch 760 loss: 0.0072382697835565\n",
      "Epoch 25, Batch 780 loss: 0.0072402856312692\n",
      "Epoch: 25 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 26, Batch 20 loss: 0.0071168183349073\n",
      "Epoch 26, Batch 40 loss: 0.0069475146010518\n",
      "Epoch 26, Batch 60 loss: 0.0071729859337211\n",
      "Epoch 26, Batch 80 loss: 0.0070463204756379\n",
      "Epoch 26, Batch 100 loss: 0.0069772265851498\n",
      "Epoch 26, Batch 120 loss: 0.0069828452542424\n",
      "Epoch 26, Batch 140 loss: 0.0069722482003272\n",
      "Epoch 26, Batch 160 loss: 0.0069396980106831\n",
      "Epoch 26, Batch 180 loss: 0.0069961338303983\n",
      "Epoch 26, Batch 200 loss: 0.0069622443988919\n",
      "Epoch 26, Batch 220 loss: 0.0070653269067407\n",
      "Epoch 26, Batch 240 loss: 0.0070716468617320\n",
      "Epoch 26, Batch 260 loss: 0.0071147545240819\n",
      "Epoch 26, Batch 280 loss: 0.0071776122786105\n",
      "Epoch 26, Batch 300 loss: 0.0071610063314438\n",
      "Epoch 26, Batch 320 loss: 0.0071489633992314\n",
      "Epoch 26, Batch 340 loss: 0.0071544065140188\n",
      "Epoch 26, Batch 360 loss: 0.0071380981244147\n",
      "Epoch 26, Batch 380 loss: 0.0071440809406340\n",
      "Epoch 26, Batch 400 loss: 0.0071853883564472\n",
      "Epoch 26, Batch 420 loss: 0.0071957493200898\n",
      "Epoch 26, Batch 440 loss: 0.0072356909513474\n",
      "Epoch 26, Batch 460 loss: 0.0072102928534150\n",
      "Epoch 26, Batch 480 loss: 0.0072352876886725\n",
      "Epoch 26, Batch 500 loss: 0.0072804256342351\n",
      "Epoch 26, Batch 520 loss: 0.0072824778035283\n",
      "Epoch 26, Batch 540 loss: 0.0072696940042078\n",
      "Epoch 26, Batch 560 loss: 0.0072918431833386\n",
      "Epoch 26, Batch 580 loss: 0.0072759776376188\n",
      "Epoch 26, Batch 600 loss: 0.0072602047584951\n",
      "Epoch 26, Batch 620 loss: 0.0072783278301358\n",
      "Epoch 26, Batch 640 loss: 0.0072684092447162\n",
      "Epoch 26, Batch 660 loss: 0.0072967344895005\n",
      "Epoch 26, Batch 680 loss: 0.0073023936711252\n",
      "Epoch 26, Batch 700 loss: 0.0072943447157741\n",
      "Epoch 26, Batch 720 loss: 0.0072739743627608\n",
      "Epoch 26, Batch 740 loss: 0.0072643845342100\n",
      "Epoch 26, Batch 760 loss: 0.0072339624166489\n",
      "Epoch 26, Batch 780 loss: 0.0072237052954733\n",
      "Epoch: 26 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 27, Batch 20 loss: 0.0070767886936665\n",
      "Epoch 27, Batch 40 loss: 0.0072140088304877\n",
      "Epoch 27, Batch 60 loss: 0.0072667831555009\n",
      "Epoch 27, Batch 80 loss: 0.0071764490567148\n",
      "Epoch 27, Batch 100 loss: 0.0069455592893064\n",
      "Epoch 27, Batch 120 loss: 0.0069473544135690\n",
      "Epoch 27, Batch 140 loss: 0.0069240196608007\n",
      "Epoch 27, Batch 160 loss: 0.0070078372955322\n",
      "Epoch 27, Batch 180 loss: 0.0070495558902621\n",
      "Epoch 27, Batch 200 loss: 0.0069962465204298\n",
      "Epoch 27, Batch 220 loss: 0.0069662830792367\n",
      "Epoch 27, Batch 240 loss: 0.0069599463604391\n",
      "Epoch 27, Batch 260 loss: 0.0070794918574393\n",
      "Epoch 27, Batch 280 loss: 0.0070809898898005\n",
      "Epoch 27, Batch 300 loss: 0.0070619285106659\n",
      "Epoch 27, Batch 320 loss: 0.0070747183635831\n",
      "Epoch 27, Batch 340 loss: 0.0070909895002842\n",
      "Epoch 27, Batch 360 loss: 0.0070499940775335\n",
      "Epoch 27, Batch 380 loss: 0.0070764226838946\n",
      "Epoch 27, Batch 400 loss: 0.0070823980495334\n",
      "Epoch 27, Batch 420 loss: 0.0070957941934466\n",
      "Epoch 27, Batch 440 loss: 0.0071287378668785\n",
      "Epoch 27, Batch 460 loss: 0.0071452632546425\n",
      "Epoch 27, Batch 480 loss: 0.0071185589767992\n",
      "Epoch 27, Batch 500 loss: 0.0071138655766845\n",
      "Epoch 27, Batch 520 loss: 0.0071075693704188\n",
      "Epoch 27, Batch 540 loss: 0.0071562179364264\n",
      "Epoch 27, Batch 560 loss: 0.0071208784356713\n",
      "Epoch 27, Batch 580 loss: 0.0071560628712177\n",
      "Epoch 27, Batch 600 loss: 0.0071205422282219\n",
      "Epoch 27, Batch 620 loss: 0.0071485838852823\n",
      "Epoch 27, Batch 640 loss: 0.0071650808677077\n",
      "Epoch 27, Batch 660 loss: 0.0071731349453330\n",
      "Epoch 27, Batch 680 loss: 0.0071624731644988\n",
      "Epoch 27, Batch 700 loss: 0.0071629635058343\n",
      "Epoch 27, Batch 720 loss: 0.0071548586711287\n",
      "Epoch 27, Batch 740 loss: 0.0071744821034372\n",
      "Epoch 27, Batch 760 loss: 0.0071653351187706\n",
      "Epoch 27, Batch 780 loss: 0.0071889171376824\n",
      "Epoch: 27 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 28, Batch 20 loss: 0.0074230553582311\n",
      "Epoch 28, Batch 40 loss: 0.0071953036822379\n",
      "Epoch 28, Batch 60 loss: 0.0073250136338174\n",
      "Epoch 28, Batch 80 loss: 0.0073681860230863\n",
      "Epoch 28, Batch 100 loss: 0.0072861714288592\n",
      "Epoch 28, Batch 120 loss: 0.0071243508718908\n",
      "Epoch 28, Batch 140 loss: 0.0072675100527704\n",
      "Epoch 28, Batch 160 loss: 0.0074081206694245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 180 loss: 0.0073699518106878\n",
      "Epoch 28, Batch 200 loss: 0.0073222480714321\n",
      "Epoch 28, Batch 220 loss: 0.0074305729940534\n",
      "Epoch 28, Batch 240 loss: 0.0073669790290296\n",
      "Epoch 28, Batch 260 loss: 0.0073175430297852\n",
      "Epoch 28, Batch 280 loss: 0.0073035955429077\n",
      "Epoch 28, Batch 300 loss: 0.0073551177047193\n",
      "Epoch 28, Batch 320 loss: 0.0072619626298547\n",
      "Epoch 28, Batch 340 loss: 0.0072495127096772\n",
      "Epoch 28, Batch 360 loss: 0.0071960673667490\n",
      "Epoch 28, Batch 380 loss: 0.0071536800824106\n",
      "Epoch 28, Batch 400 loss: 0.0071076177991927\n",
      "Epoch 28, Batch 420 loss: 0.0070475190877914\n",
      "Epoch 28, Batch 440 loss: 0.0070272600278258\n",
      "Epoch 28, Batch 460 loss: 0.0069765760563314\n",
      "Epoch 28, Batch 480 loss: 0.0069316090084612\n",
      "Epoch 28, Batch 500 loss: 0.0069550261832774\n",
      "Epoch 28, Batch 520 loss: 0.0069876848720014\n",
      "Epoch 28, Batch 540 loss: 0.0069858757779002\n",
      "Epoch 28, Batch 560 loss: 0.0069988900795579\n",
      "Epoch 28, Batch 580 loss: 0.0069920113310218\n",
      "Epoch 28, Batch 600 loss: 0.0070034214295447\n",
      "Epoch 28, Batch 620 loss: 0.0070190080441535\n",
      "Epoch 28, Batch 640 loss: 0.0070143663324416\n",
      "Epoch 28, Batch 660 loss: 0.0070546036586165\n",
      "Epoch 28, Batch 680 loss: 0.0070656528696418\n",
      "Epoch 28, Batch 700 loss: 0.0070984540507197\n",
      "Epoch 28, Batch 720 loss: 0.0071329823695123\n",
      "Epoch 28, Batch 740 loss: 0.0071627134457231\n",
      "Epoch 28, Batch 760 loss: 0.0071408213116229\n",
      "Epoch 28, Batch 780 loss: 0.0071123158559203\n",
      "Epoch: 28 \tTraining Loss: 0.000003 \tValidation Loss: 0.000014\n",
      "Validation loss decreased (0.000015 --> 0.000014).  Saving model ...\n",
      "Epoch 29, Batch 20 loss: 0.0075403377413750\n",
      "Epoch 29, Batch 40 loss: 0.0066463360562921\n",
      "Epoch 29, Batch 60 loss: 0.0070692016743124\n",
      "Epoch 29, Batch 80 loss: 0.0070307264104486\n",
      "Epoch 29, Batch 100 loss: 0.0069204508326948\n",
      "Epoch 29, Batch 120 loss: 0.0069071254692972\n",
      "Epoch 29, Batch 140 loss: 0.0067637474276125\n",
      "Epoch 29, Batch 160 loss: 0.0067974580451846\n",
      "Epoch 29, Batch 180 loss: 0.0067256488837302\n",
      "Epoch 29, Batch 200 loss: 0.0067442739382386\n",
      "Epoch 29, Batch 220 loss: 0.0067626619711518\n",
      "Epoch 29, Batch 240 loss: 0.0068304585292935\n",
      "Epoch 29, Batch 260 loss: 0.0068857781589031\n",
      "Epoch 29, Batch 280 loss: 0.0068400613963604\n",
      "Epoch 29, Batch 300 loss: 0.0068180100060999\n",
      "Epoch 29, Batch 320 loss: 0.0069021694362164\n",
      "Epoch 29, Batch 340 loss: 0.0069464789703488\n",
      "Epoch 29, Batch 360 loss: 0.0069248988293111\n",
      "Epoch 29, Batch 380 loss: 0.0069542964920402\n",
      "Epoch 29, Batch 400 loss: 0.0069123096764088\n",
      "Epoch 29, Batch 420 loss: 0.0069243521429598\n",
      "Epoch 29, Batch 440 loss: 0.0069471858441830\n",
      "Epoch 29, Batch 460 loss: 0.0069503351114690\n",
      "Epoch 29, Batch 480 loss: 0.0069636153057218\n",
      "Epoch 29, Batch 500 loss: 0.0070000737905502\n",
      "Epoch 29, Batch 520 loss: 0.0070101819001138\n",
      "Epoch 29, Batch 540 loss: 0.0070129125379026\n",
      "Epoch 29, Batch 560 loss: 0.0069830515421927\n",
      "Epoch 29, Batch 580 loss: 0.0069904937408864\n",
      "Epoch 29, Batch 600 loss: 0.0070417374372482\n",
      "Epoch 29, Batch 620 loss: 0.0070425434969366\n",
      "Epoch 29, Batch 640 loss: 0.0070340028032660\n",
      "Epoch 29, Batch 660 loss: 0.0070227175019681\n",
      "Epoch 29, Batch 680 loss: 0.0070449351333082\n",
      "Epoch 29, Batch 700 loss: 0.0070711197331548\n",
      "Epoch 29, Batch 720 loss: 0.0070794494822621\n",
      "Epoch 29, Batch 740 loss: 0.0070915818214417\n",
      "Epoch 29, Batch 760 loss: 0.0070976065471768\n",
      "Epoch 29, Batch 780 loss: 0.0071084173396230\n",
      "Epoch: 29 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 30, Batch 20 loss: 0.0069756493903697\n",
      "Epoch 30, Batch 40 loss: 0.0069973096251488\n",
      "Epoch 30, Batch 60 loss: 0.0072669200599194\n",
      "Epoch 30, Batch 80 loss: 0.0074480660259724\n",
      "Epoch 30, Batch 100 loss: 0.0073284646496177\n",
      "Epoch 30, Batch 120 loss: 0.0072179972194135\n",
      "Epoch 30, Batch 140 loss: 0.0072251344099641\n",
      "Epoch 30, Batch 160 loss: 0.0071326205506921\n",
      "Epoch 30, Batch 180 loss: 0.0071496376767755\n",
      "Epoch 30, Batch 200 loss: 0.0071122371591628\n",
      "Epoch 30, Batch 220 loss: 0.0071267886087298\n",
      "Epoch 30, Batch 240 loss: 0.0070525482296944\n",
      "Epoch 30, Batch 260 loss: 0.0070701041258872\n",
      "Epoch 30, Batch 280 loss: 0.0070342989638448\n",
      "Epoch 30, Batch 300 loss: 0.0069849789142609\n",
      "Epoch 30, Batch 320 loss: 0.0069987634196877\n",
      "Epoch 30, Batch 340 loss: 0.0070165917277336\n",
      "Epoch 30, Batch 360 loss: 0.0070957811549306\n",
      "Epoch 30, Batch 380 loss: 0.0070681883953512\n",
      "Epoch 30, Batch 400 loss: 0.0070646852254868\n",
      "Epoch 30, Batch 420 loss: 0.0071048834361136\n",
      "Epoch 30, Batch 440 loss: 0.0070884739980102\n",
      "Epoch 30, Batch 460 loss: 0.0070971674285829\n",
      "Epoch 30, Batch 480 loss: 0.0071049602702260\n",
      "Epoch 30, Batch 500 loss: 0.0071030766703188\n",
      "Epoch 30, Batch 520 loss: 0.0070760934613645\n",
      "Epoch 30, Batch 540 loss: 0.0070554101839662\n",
      "Epoch 30, Batch 560 loss: 0.0070331832394004\n",
      "Epoch 30, Batch 580 loss: 0.0070356046780944\n",
      "Epoch 30, Batch 600 loss: 0.0070441858842969\n",
      "Epoch 30, Batch 620 loss: 0.0070307394489646\n",
      "Epoch 30, Batch 640 loss: 0.0070208334363997\n",
      "Epoch 30, Batch 660 loss: 0.0070583312772214\n",
      "Epoch 30, Batch 680 loss: 0.0070632584393024\n",
      "Epoch 30, Batch 700 loss: 0.0071055516600609\n",
      "Epoch 30, Batch 720 loss: 0.0071305995807052\n",
      "Epoch 30, Batch 740 loss: 0.0071354894898832\n",
      "Epoch 30, Batch 760 loss: 0.0071319900453091\n",
      "Epoch 30, Batch 780 loss: 0.0071139642968774\n",
      "Epoch: 30 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 31, Batch 20 loss: 0.0055475961416960\n",
      "Epoch 31, Batch 40 loss: 0.0065258061513305\n",
      "Epoch 31, Batch 60 loss: 0.0069625182077289\n",
      "Epoch 31, Batch 80 loss: 0.0069343643262982\n",
      "Epoch 31, Batch 100 loss: 0.0070280404761434\n",
      "Epoch 31, Batch 120 loss: 0.0071302042342722\n",
      "Epoch 31, Batch 140 loss: 0.0070734298788011\n",
      "Epoch 31, Batch 160 loss: 0.0070017790421844\n",
      "Epoch 31, Batch 180 loss: 0.0069633810780942\n",
      "Epoch 31, Batch 200 loss: 0.0070149442180991\n",
      "Epoch 31, Batch 220 loss: 0.0070349611341953\n",
      "Epoch 31, Batch 240 loss: 0.0070585729554296\n",
      "Epoch 31, Batch 260 loss: 0.0071033509448171\n",
      "Epoch 31, Batch 280 loss: 0.0071070371195674\n",
      "Epoch 31, Batch 300 loss: 0.0071262167766690\n",
      "Epoch 31, Batch 320 loss: 0.0071407854557037\n",
      "Epoch 31, Batch 340 loss: 0.0071183233521879\n",
      "Epoch 31, Batch 360 loss: 0.0071235499344766\n",
      "Epoch 31, Batch 380 loss: 0.0071004889905453\n",
      "Epoch 31, Batch 400 loss: 0.0071158297359943\n",
      "Epoch 31, Batch 420 loss: 0.0071477121673524\n",
      "Epoch 31, Batch 440 loss: 0.0071534803137183\n",
      "Epoch 31, Batch 460 loss: 0.0071671842597425\n",
      "Epoch 31, Batch 480 loss: 0.0071557918563485\n",
      "Epoch 31, Batch 500 loss: 0.0071199676021934\n",
      "Epoch 31, Batch 520 loss: 0.0071447426453233\n",
      "Epoch 31, Batch 540 loss: 0.0071526663377881\n",
      "Epoch 31, Batch 560 loss: 0.0071507529355586\n",
      "Epoch 31, Batch 580 loss: 0.0071945264935493\n",
      "Epoch 31, Batch 600 loss: 0.0071776993572712\n",
      "Epoch 31, Batch 620 loss: 0.0072062076069415\n",
      "Epoch 31, Batch 640 loss: 0.0071680857799947\n",
      "Epoch 31, Batch 660 loss: 0.0071480600163341\n",
      "Epoch 31, Batch 680 loss: 0.0071408278308809\n",
      "Epoch 31, Batch 700 loss: 0.0071070240810513\n",
      "Epoch 31, Batch 720 loss: 0.0071088122203946\n",
      "Epoch 31, Batch 740 loss: 0.0071036852896214\n",
      "Epoch 31, Batch 760 loss: 0.0071343877352774\n",
      "Epoch 31, Batch 780 loss: 0.0071230032481253\n",
      "Epoch: 31 \tTraining Loss: 0.000003 \tValidation Loss: 0.000014\n",
      "Validation loss decreased (0.000014 --> 0.000014).  Saving model ...\n",
      "Epoch 32, Batch 20 loss: 0.0056417644955218\n",
      "Epoch 32, Batch 40 loss: 0.0060998750850558\n",
      "Epoch 32, Batch 60 loss: 0.0064809895120561\n",
      "Epoch 32, Batch 80 loss: 0.0065463446080685\n",
      "Epoch 32, Batch 100 loss: 0.0065794857218862\n",
      "Epoch 32, Batch 120 loss: 0.0067736702039838\n",
      "Epoch 32, Batch 140 loss: 0.0069063180126250\n",
      "Epoch 32, Batch 160 loss: 0.0067988419905305\n",
      "Epoch 32, Batch 180 loss: 0.0067617148160934\n",
      "Epoch 32, Batch 200 loss: 0.0067683560773730\n",
      "Epoch 32, Batch 220 loss: 0.0067524416372180\n",
      "Epoch 32, Batch 240 loss: 0.0067397495731711\n",
      "Epoch 32, Batch 260 loss: 0.0066715292632580\n",
      "Epoch 32, Batch 280 loss: 0.0066606486216187\n",
      "Epoch 32, Batch 300 loss: 0.0066836155019701\n",
      "Epoch 32, Batch 320 loss: 0.0066925836727023\n",
      "Epoch 32, Batch 340 loss: 0.0067241275683045\n",
      "Epoch 32, Batch 360 loss: 0.0068001700565219\n",
      "Epoch 32, Batch 380 loss: 0.0068321996368468\n",
      "Epoch 32, Batch 400 loss: 0.0068404367193580\n",
      "Epoch 32, Batch 420 loss: 0.0068872855044901\n",
      "Epoch 32, Batch 440 loss: 0.0068772351369262\n",
      "Epoch 32, Batch 460 loss: 0.0069048525765538\n",
      "Epoch 32, Batch 480 loss: 0.0069321915507317\n",
      "Epoch 32, Batch 500 loss: 0.0069315470755100\n",
      "Epoch 32, Batch 520 loss: 0.0069779655896127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 540 loss: 0.0069883293472230\n",
      "Epoch 32, Batch 560 loss: 0.0070112445391715\n",
      "Epoch 32, Batch 580 loss: 0.0070382319390774\n",
      "Epoch 32, Batch 600 loss: 0.0070504010654986\n",
      "Epoch 32, Batch 620 loss: 0.0070281527005136\n",
      "Epoch 32, Batch 640 loss: 0.0070370333269238\n",
      "Epoch 32, Batch 660 loss: 0.0070340074598789\n",
      "Epoch 32, Batch 680 loss: 0.0070657096803188\n",
      "Epoch 32, Batch 700 loss: 0.0070432834327221\n",
      "Epoch 32, Batch 720 loss: 0.0070437728427351\n",
      "Epoch 32, Batch 740 loss: 0.0070378379896283\n",
      "Epoch 32, Batch 760 loss: 0.0070418976247311\n",
      "Epoch 32, Batch 780 loss: 0.0070384866558015\n",
      "Epoch: 32 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 33, Batch 20 loss: 0.0071096653118730\n",
      "Epoch 33, Batch 40 loss: 0.0069814957678318\n",
      "Epoch 33, Batch 60 loss: 0.0067069320939481\n",
      "Epoch 33, Batch 80 loss: 0.0068228794261813\n",
      "Epoch 33, Batch 100 loss: 0.0066490271128714\n",
      "Epoch 33, Batch 120 loss: 0.0067156767472625\n",
      "Epoch 33, Batch 140 loss: 0.0068811527453363\n",
      "Epoch 33, Batch 160 loss: 0.0069884969852865\n",
      "Epoch 33, Batch 180 loss: 0.0069677392020822\n",
      "Epoch 33, Batch 200 loss: 0.0070110643282533\n",
      "Epoch 33, Batch 220 loss: 0.0070690824650228\n",
      "Epoch 33, Batch 240 loss: 0.0070929587818682\n",
      "Epoch 33, Batch 260 loss: 0.0070779710076749\n",
      "Epoch 33, Batch 280 loss: 0.0070613892748952\n",
      "Epoch 33, Batch 300 loss: 0.0070610223338008\n",
      "Epoch 33, Batch 320 loss: 0.0070989886298776\n",
      "Epoch 33, Batch 340 loss: 0.0070768445730209\n",
      "Epoch 33, Batch 360 loss: 0.0071143163368106\n",
      "Epoch 33, Batch 380 loss: 0.0071061877533793\n",
      "Epoch 33, Batch 400 loss: 0.0071738227270544\n",
      "Epoch 33, Batch 420 loss: 0.0072141475975513\n",
      "Epoch 33, Batch 440 loss: 0.0071936785243452\n",
      "Epoch 33, Batch 460 loss: 0.0071755247190595\n",
      "Epoch 33, Batch 480 loss: 0.0072291470132768\n",
      "Epoch 33, Batch 500 loss: 0.0071979998610914\n",
      "Epoch 33, Batch 520 loss: 0.0071909772232175\n",
      "Epoch 33, Batch 540 loss: 0.0071665709838271\n",
      "Epoch 33, Batch 560 loss: 0.0071146981790662\n",
      "Epoch 33, Batch 580 loss: 0.0070996703580022\n",
      "Epoch 33, Batch 600 loss: 0.0071097202599049\n",
      "Epoch 33, Batch 620 loss: 0.0070774992927909\n",
      "Epoch 33, Batch 640 loss: 0.0070632034912705\n",
      "Epoch 33, Batch 660 loss: 0.0070651331916451\n",
      "Epoch 33, Batch 680 loss: 0.0070652244612575\n",
      "Epoch 33, Batch 700 loss: 0.0070925331674516\n",
      "Epoch 33, Batch 720 loss: 0.0071022743359208\n",
      "Epoch 33, Batch 740 loss: 0.0070924861356616\n",
      "Epoch 33, Batch 760 loss: 0.0070996447466314\n",
      "Epoch 33, Batch 780 loss: 0.0070940656587481\n",
      "Epoch: 33 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 34, Batch 20 loss: 0.0067008016631007\n",
      "Epoch 34, Batch 40 loss: 0.0069056255742908\n",
      "Epoch 34, Batch 60 loss: 0.0072506689466536\n",
      "Epoch 34, Batch 80 loss: 0.0071556367911398\n",
      "Epoch 34, Batch 100 loss: 0.0072603286243975\n",
      "Epoch 34, Batch 120 loss: 0.0071224034763873\n",
      "Epoch 34, Batch 140 loss: 0.0070279524661601\n",
      "Epoch 34, Batch 160 loss: 0.0069925440475345\n",
      "Epoch 34, Batch 180 loss: 0.0068884110078216\n",
      "Epoch 34, Batch 200 loss: 0.0068405335769057\n",
      "Epoch 34, Batch 220 loss: 0.0068846838548779\n",
      "Epoch 34, Batch 240 loss: 0.0069546969607472\n",
      "Epoch 34, Batch 260 loss: 0.0069220187142491\n",
      "Epoch 34, Batch 280 loss: 0.0069606914184988\n",
      "Epoch 34, Batch 300 loss: 0.0069453194737434\n",
      "Epoch 34, Batch 320 loss: 0.0069791153073311\n",
      "Epoch 34, Batch 340 loss: 0.0070150359533727\n",
      "Epoch 34, Batch 360 loss: 0.0070417164824903\n",
      "Epoch 34, Batch 380 loss: 0.0070787244476378\n",
      "Epoch 34, Batch 400 loss: 0.0071315742097795\n",
      "Epoch 34, Batch 420 loss: 0.0071399696171284\n",
      "Epoch 34, Batch 440 loss: 0.0071474844589829\n",
      "Epoch 34, Batch 460 loss: 0.0071016526781023\n",
      "Epoch 34, Batch 480 loss: 0.0070882528088987\n",
      "Epoch 34, Batch 500 loss: 0.0070899664424360\n",
      "Epoch 34, Batch 520 loss: 0.0070676528848708\n",
      "Epoch 34, Batch 540 loss: 0.0070618614554405\n",
      "Epoch 34, Batch 560 loss: 0.0070400671102107\n",
      "Epoch 34, Batch 580 loss: 0.0070499731227756\n",
      "Epoch 34, Batch 600 loss: 0.0070845559239388\n",
      "Epoch 34, Batch 620 loss: 0.0071146944537759\n",
      "Epoch 34, Batch 640 loss: 0.0071121291257441\n",
      "Epoch 34, Batch 660 loss: 0.0071264049038291\n",
      "Epoch 34, Batch 680 loss: 0.0071193575859070\n",
      "Epoch 34, Batch 700 loss: 0.0070860995911062\n",
      "Epoch 34, Batch 720 loss: 0.0071020117029548\n",
      "Epoch 34, Batch 740 loss: 0.0071236141957343\n",
      "Epoch 34, Batch 760 loss: 0.0071267485618591\n",
      "Epoch 34, Batch 780 loss: 0.0071147955022752\n",
      "Epoch: 34 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n",
      "Epoch 35, Batch 20 loss: 0.0075708748772740\n",
      "Epoch 35, Batch 40 loss: 0.0074409125372767\n",
      "Epoch 35, Batch 60 loss: 0.0071121654473245\n",
      "Epoch 35, Batch 80 loss: 0.0069404589012265\n",
      "Epoch 35, Batch 100 loss: 0.0071339285932481\n",
      "Epoch 35, Batch 120 loss: 0.0071592419408262\n",
      "Epoch 35, Batch 140 loss: 0.0070772888138890\n",
      "Epoch 35, Batch 160 loss: 0.0070887738838792\n",
      "Epoch 35, Batch 180 loss: 0.0071590193547308\n",
      "Epoch 35, Batch 200 loss: 0.0071603870019317\n",
      "Epoch 35, Batch 220 loss: 0.0071121295914054\n",
      "Epoch 35, Batch 240 loss: 0.0070229554548860\n",
      "Epoch 35, Batch 260 loss: 0.0070111290551722\n",
      "Epoch 35, Batch 280 loss: 0.0070782750844955\n",
      "Epoch 35, Batch 300 loss: 0.0070398887619376\n",
      "Epoch 35, Batch 320 loss: 0.0069633862003684\n",
      "Epoch 35, Batch 340 loss: 0.0070001101121306\n",
      "Epoch 35, Batch 360 loss: 0.0070694684982300\n",
      "Epoch 35, Batch 380 loss: 0.0070489533245564\n",
      "Epoch 35, Batch 400 loss: 0.0070280581712723\n",
      "Epoch 35, Batch 420 loss: 0.0070231766439974\n",
      "Epoch 35, Batch 440 loss: 0.0069936127401888\n",
      "Epoch 35, Batch 460 loss: 0.0069964132271707\n",
      "Epoch 35, Batch 480 loss: 0.0070009329356253\n",
      "Epoch 35, Batch 500 loss: 0.0070150950923562\n",
      "Epoch 35, Batch 520 loss: 0.0070247659459710\n",
      "Epoch 35, Batch 540 loss: 0.0070066400803626\n",
      "Epoch 35, Batch 560 loss: 0.0070390389300883\n",
      "Epoch 35, Batch 580 loss: 0.0070888362824917\n",
      "Epoch 35, Batch 600 loss: 0.0070600211620331\n",
      "Epoch 35, Batch 620 loss: 0.0070557272993028\n",
      "Epoch 35, Batch 640 loss: 0.0070611350238323\n",
      "Epoch 35, Batch 660 loss: 0.0070634149014950\n",
      "Epoch 35, Batch 680 loss: 0.0070593045093119\n",
      "Epoch 35, Batch 700 loss: 0.0070679485797882\n",
      "Epoch 35, Batch 720 loss: 0.0070340400561690\n",
      "Epoch 35, Batch 740 loss: 0.0070434315130115\n",
      "Epoch 35, Batch 760 loss: 0.0070379814133048\n",
      "Epoch 35, Batch 780 loss: 0.0070472611114383\n",
      "Epoch: 35 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 36, Batch 20 loss: 0.0065628020092845\n",
      "Epoch 36, Batch 40 loss: 0.0067118452861905\n",
      "Epoch 36, Batch 60 loss: 0.0067674936726689\n",
      "Epoch 36, Batch 80 loss: 0.0069890744052827\n",
      "Epoch 36, Batch 100 loss: 0.0072306441143155\n",
      "Epoch 36, Batch 120 loss: 0.0074706329032779\n",
      "Epoch 36, Batch 140 loss: 0.0073626884259284\n",
      "Epoch 36, Batch 160 loss: 0.0072340667247772\n",
      "Epoch 36, Batch 180 loss: 0.0071267420426011\n",
      "Epoch 36, Batch 200 loss: 0.0070624551735818\n",
      "Epoch 36, Batch 220 loss: 0.0072073922492564\n",
      "Epoch 36, Batch 240 loss: 0.0071771987713873\n",
      "Epoch 36, Batch 260 loss: 0.0071183517575264\n",
      "Epoch 36, Batch 280 loss: 0.0071128578856587\n",
      "Epoch 36, Batch 300 loss: 0.0071095703169703\n",
      "Epoch 36, Batch 320 loss: 0.0071402648463845\n",
      "Epoch 36, Batch 340 loss: 0.0071457228623331\n",
      "Epoch 36, Batch 360 loss: 0.0071381689049304\n",
      "Epoch 36, Batch 380 loss: 0.0071348175406456\n",
      "Epoch 36, Batch 400 loss: 0.0070929056964815\n",
      "Epoch 36, Batch 420 loss: 0.0071149906143546\n",
      "Epoch 36, Batch 440 loss: 0.0071318456903100\n",
      "Epoch 36, Batch 460 loss: 0.0071296901442111\n",
      "Epoch 36, Batch 480 loss: 0.0071216365322471\n",
      "Epoch 36, Batch 500 loss: 0.0071400613524020\n",
      "Epoch 36, Batch 520 loss: 0.0071195675991476\n",
      "Epoch 36, Batch 540 loss: 0.0071193627081811\n",
      "Epoch 36, Batch 560 loss: 0.0070826606824994\n",
      "Epoch 36, Batch 580 loss: 0.0070978454314172\n",
      "Epoch 36, Batch 600 loss: 0.0071202581748366\n",
      "Epoch 36, Batch 620 loss: 0.0071297087706625\n",
      "Epoch 36, Batch 640 loss: 0.0071242162957788\n",
      "Epoch 36, Batch 660 loss: 0.0071047297678888\n",
      "Epoch 36, Batch 680 loss: 0.0071182311512530\n",
      "Epoch 36, Batch 700 loss: 0.0071221501566470\n",
      "Epoch 36, Batch 720 loss: 0.0071050054393709\n",
      "Epoch 36, Batch 740 loss: 0.0071120583452284\n",
      "Epoch 36, Batch 760 loss: 0.0071089663542807\n",
      "Epoch 36, Batch 780 loss: 0.0071317567490041\n",
      "Epoch: 36 \tTraining Loss: 0.000003 \tValidation Loss: 0.000018\n",
      "Epoch 37, Batch 20 loss: 0.0076167634688318\n",
      "Epoch 37, Batch 40 loss: 0.0073647387325764\n",
      "Epoch 37, Batch 60 loss: 0.0075144944712520\n",
      "Epoch 37, Batch 80 loss: 0.0072523197159171\n",
      "Epoch 37, Batch 100 loss: 0.0073839821852744\n",
      "Epoch 37, Batch 120 loss: 0.0074522495269775\n",
      "Epoch 37, Batch 140 loss: 0.0072723166085780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 160 loss: 0.0073035159148276\n",
      "Epoch 37, Batch 180 loss: 0.0073193847201765\n",
      "Epoch 37, Batch 200 loss: 0.0072818025946617\n",
      "Epoch 37, Batch 220 loss: 0.0072623142041266\n",
      "Epoch 37, Batch 240 loss: 0.0073105795308948\n",
      "Epoch 37, Batch 260 loss: 0.0073574818670750\n",
      "Epoch 37, Batch 280 loss: 0.0072343396022916\n",
      "Epoch 37, Batch 300 loss: 0.0072271814569831\n",
      "Epoch 37, Batch 320 loss: 0.0071762017905712\n",
      "Epoch 37, Batch 340 loss: 0.0071540540084243\n",
      "Epoch 37, Batch 360 loss: 0.0071184858679771\n",
      "Epoch 37, Batch 380 loss: 0.0071337088011205\n",
      "Epoch 37, Batch 400 loss: 0.0071293944492936\n",
      "Epoch 37, Batch 420 loss: 0.0070923017337918\n",
      "Epoch 37, Batch 440 loss: 0.0071288296021521\n",
      "Epoch 37, Batch 460 loss: 0.0071241571567953\n",
      "Epoch 37, Batch 480 loss: 0.0071012279950082\n",
      "Epoch 37, Batch 500 loss: 0.0071287518367171\n",
      "Epoch 37, Batch 520 loss: 0.0071421773172915\n",
      "Epoch 37, Batch 540 loss: 0.0070965485647321\n",
      "Epoch 37, Batch 560 loss: 0.0070875273086131\n",
      "Epoch 37, Batch 580 loss: 0.0070808148011565\n",
      "Epoch 37, Batch 600 loss: 0.0071004712954164\n",
      "Epoch 37, Batch 620 loss: 0.0071122394874692\n",
      "Epoch 37, Batch 640 loss: 0.0070950267836452\n",
      "Epoch 37, Batch 660 loss: 0.0070979380980134\n",
      "Epoch 37, Batch 680 loss: 0.0070846118032932\n",
      "Epoch 37, Batch 700 loss: 0.0070425868034363\n",
      "Epoch 37, Batch 720 loss: 0.0070247175171971\n",
      "Epoch 37, Batch 740 loss: 0.0070506534539163\n",
      "Epoch 37, Batch 760 loss: 0.0070499866269529\n",
      "Epoch 37, Batch 780 loss: 0.0070367418229580\n",
      "Epoch: 37 \tTraining Loss: 0.000003 \tValidation Loss: 0.000017\n",
      "Epoch 38, Batch 20 loss: 0.0061182519420981\n",
      "Epoch 38, Batch 40 loss: 0.0065109110437334\n",
      "Epoch 38, Batch 60 loss: 0.0066030481830239\n",
      "Epoch 38, Batch 80 loss: 0.0067919702269137\n",
      "Epoch 38, Batch 100 loss: 0.0068629411980510\n",
      "Epoch 38, Batch 120 loss: 0.0068178973160684\n",
      "Epoch 38, Batch 140 loss: 0.0068658068776131\n",
      "Epoch 38, Batch 160 loss: 0.0067643299698830\n",
      "Epoch 38, Batch 180 loss: 0.0068152910098433\n",
      "Epoch 38, Batch 200 loss: 0.0067288903519511\n",
      "Epoch 38, Batch 220 loss: 0.0067404410801828\n",
      "Epoch 38, Batch 240 loss: 0.0067938752472401\n",
      "Epoch 38, Batch 260 loss: 0.0068052499555051\n",
      "Epoch 38, Batch 280 loss: 0.0068684285506606\n",
      "Epoch 38, Batch 300 loss: 0.0068097324110568\n",
      "Epoch 38, Batch 320 loss: 0.0068343491293490\n",
      "Epoch 38, Batch 340 loss: 0.0068418718874454\n",
      "Epoch 38, Batch 360 loss: 0.0068833762779832\n",
      "Epoch 38, Batch 380 loss: 0.0068656541407108\n",
      "Epoch 38, Batch 400 loss: 0.0068932301364839\n",
      "Epoch 38, Batch 420 loss: 0.0069127068854868\n",
      "Epoch 38, Batch 440 loss: 0.0069343312643468\n",
      "Epoch 38, Batch 460 loss: 0.0069723851047456\n",
      "Epoch 38, Batch 480 loss: 0.0069782300852239\n",
      "Epoch 38, Batch 500 loss: 0.0070003853179514\n",
      "Epoch 38, Batch 520 loss: 0.0069876364432275\n",
      "Epoch 38, Batch 540 loss: 0.0070207668468356\n",
      "Epoch 38, Batch 560 loss: 0.0070835659280419\n",
      "Epoch 38, Batch 580 loss: 0.0070736706256866\n",
      "Epoch 38, Batch 600 loss: 0.0070683448575437\n",
      "Epoch 38, Batch 620 loss: 0.0070347674190998\n",
      "Epoch 38, Batch 640 loss: 0.0070581557229161\n",
      "Epoch 38, Batch 660 loss: 0.0070570111274719\n",
      "Epoch 38, Batch 680 loss: 0.0070576258003712\n",
      "Epoch 38, Batch 700 loss: 0.0071091568097472\n",
      "Epoch 38, Batch 720 loss: 0.0071146572008729\n",
      "Epoch 38, Batch 740 loss: 0.0071144253015518\n",
      "Epoch 38, Batch 760 loss: 0.0071115149185061\n",
      "Epoch 38, Batch 780 loss: 0.0070895515382290\n",
      "Epoch: 38 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 39, Batch 20 loss: 0.0050489227287471\n",
      "Epoch 39, Batch 40 loss: 0.0058640432544053\n",
      "Epoch 39, Batch 60 loss: 0.0063325003720820\n",
      "Epoch 39, Batch 80 loss: 0.0065129352733493\n",
      "Epoch 39, Batch 100 loss: 0.0065990760922432\n",
      "Epoch 39, Batch 120 loss: 0.0066858730278909\n",
      "Epoch 39, Batch 140 loss: 0.0067528323270380\n",
      "Epoch 39, Batch 160 loss: 0.0068517192266881\n",
      "Epoch 39, Batch 180 loss: 0.0068907998502254\n",
      "Epoch 39, Batch 200 loss: 0.0069712996482849\n",
      "Epoch 39, Batch 220 loss: 0.0070346510037780\n",
      "Epoch 39, Batch 240 loss: 0.0071244998835027\n",
      "Epoch 39, Batch 260 loss: 0.0071661332622170\n",
      "Epoch 39, Batch 280 loss: 0.0071783354505897\n",
      "Epoch 39, Batch 300 loss: 0.0071985395625234\n",
      "Epoch 39, Batch 320 loss: 0.0071784229949117\n",
      "Epoch 39, Batch 340 loss: 0.0071888989768922\n",
      "Epoch 39, Batch 360 loss: 0.0071481936611235\n",
      "Epoch 39, Batch 380 loss: 0.0071225194260478\n",
      "Epoch 39, Batch 400 loss: 0.0071005113422871\n",
      "Epoch 39, Batch 420 loss: 0.0070881592109799\n",
      "Epoch 39, Batch 440 loss: 0.0070167095400393\n",
      "Epoch 39, Batch 460 loss: 0.0069666230119765\n",
      "Epoch 39, Batch 480 loss: 0.0069392537698150\n",
      "Epoch 39, Batch 500 loss: 0.0068896962329745\n",
      "Epoch 39, Batch 520 loss: 0.0068761194124818\n",
      "Epoch 39, Batch 540 loss: 0.0069019896909595\n",
      "Epoch 39, Batch 560 loss: 0.0069182207807899\n",
      "Epoch 39, Batch 580 loss: 0.0069540599361062\n",
      "Epoch 39, Batch 600 loss: 0.0069441655650735\n",
      "Epoch 39, Batch 620 loss: 0.0069693126715720\n",
      "Epoch 39, Batch 640 loss: 0.0069895610213280\n",
      "Epoch 39, Batch 660 loss: 0.0069936243817210\n",
      "Epoch 39, Batch 680 loss: 0.0070048794150352\n",
      "Epoch 39, Batch 700 loss: 0.0069979727268219\n",
      "Epoch 39, Batch 720 loss: 0.0069963200949132\n",
      "Epoch 39, Batch 740 loss: 0.0069978954270482\n",
      "Epoch 39, Batch 760 loss: 0.0069773062132299\n",
      "Epoch 39, Batch 780 loss: 0.0069856606423855\n",
      "Epoch: 39 \tTraining Loss: 0.000003 \tValidation Loss: 0.000015\n",
      "Epoch 40, Batch 20 loss: 0.0068243122659624\n",
      "Epoch 40, Batch 40 loss: 0.0071356510743499\n",
      "Epoch 40, Batch 60 loss: 0.0068481005728245\n",
      "Epoch 40, Batch 80 loss: 0.0067929686047137\n",
      "Epoch 40, Batch 100 loss: 0.0068848142400384\n",
      "Epoch 40, Batch 120 loss: 0.0067778229713440\n",
      "Epoch 40, Batch 140 loss: 0.0068747503682971\n",
      "Epoch 40, Batch 160 loss: 0.0069826012477279\n",
      "Epoch 40, Batch 180 loss: 0.0070169954560697\n",
      "Epoch 40, Batch 200 loss: 0.0069843358360231\n",
      "Epoch 40, Batch 220 loss: 0.0070289792492986\n",
      "Epoch 40, Batch 240 loss: 0.0069962656125426\n",
      "Epoch 40, Batch 260 loss: 0.0069596357643604\n",
      "Epoch 40, Batch 280 loss: 0.0069458321668208\n",
      "Epoch 40, Batch 300 loss: 0.0069495821371675\n",
      "Epoch 40, Batch 320 loss: 0.0069633685052395\n",
      "Epoch 40, Batch 340 loss: 0.0069484813138843\n",
      "Epoch 40, Batch 360 loss: 0.0069151977077127\n",
      "Epoch 40, Batch 380 loss: 0.0069526010192931\n",
      "Epoch 40, Batch 400 loss: 0.0069607300683856\n",
      "Epoch 40, Batch 420 loss: 0.0069410935975611\n",
      "Epoch 40, Batch 440 loss: 0.0069552948698401\n",
      "Epoch 40, Batch 460 loss: 0.0069296904839575\n",
      "Epoch 40, Batch 480 loss: 0.0069284811615944\n",
      "Epoch 40, Batch 500 loss: 0.0069023394025862\n",
      "Epoch 40, Batch 520 loss: 0.0069328024983406\n",
      "Epoch 40, Batch 540 loss: 0.0069555304944515\n",
      "Epoch 40, Batch 560 loss: 0.0069351121783257\n",
      "Epoch 40, Batch 580 loss: 0.0069153485819697\n",
      "Epoch 40, Batch 600 loss: 0.0069516054354608\n",
      "Epoch 40, Batch 620 loss: 0.0069778459146619\n",
      "Epoch 40, Batch 640 loss: 0.0069869733415544\n",
      "Epoch 40, Batch 660 loss: 0.0069881244562566\n",
      "Epoch 40, Batch 680 loss: 0.0069896169006824\n",
      "Epoch 40, Batch 700 loss: 0.0069827996194363\n",
      "Epoch 40, Batch 720 loss: 0.0069846273399889\n",
      "Epoch 40, Batch 740 loss: 0.0069987825118005\n",
      "Epoch 40, Batch 760 loss: 0.0070004900917411\n",
      "Epoch 40, Batch 780 loss: 0.0070042647421360\n",
      "Epoch: 40 \tTraining Loss: 0.000003 \tValidation Loss: 0.000016\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = train(40, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model_mnist.pt')\n",
    "\n",
    "# # load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model_mnist.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.135707\n",
      "\n",
      "\n",
      "Test Accuracy: 96% (9633/10000)\n"
     ]
    }
   ],
   "source": [
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
